# 考虑大型集群

一个集群是由一系列运行k8s客户端的节点（物理机器或者虚拟机）组成，该集群受到控制平面的管理，k8s1.20版本支持高达5000个节点的集群。更具体地来说，k8s基于以下标准设计：
* 每个节点不超过100个容器组
* 集群不超过5000个节点
* 集群总共不超过150000个容器组
* 集群总共不抄错300000个容器
您可以通过增加或者移除节点实现集群的伸缩，能否达成该点取决于集群的部署方式。

## 云服务厂商的资源限制

为了避免遇到云服务提供上限额的问题，当常见多节点的集群时候，需要考虑以下几点：
* 为云资源请求增长的限额，例如：计算实例，CPU核数，存储卷，固定IP，数据包过滤规则，负载均衡器，子网段数量，日志流；
* 为增加新节点预留空间，因为一些云服务厂商会限制新建实例的速率。

## 控制平面组件

对于大型集群来说，你需要一个拥有强大算力和其他资源的控制平面。一般情况下，你需要为每一个故障域运行1个到2个控制平面。需要时，先对实例做垂直缩放，当达到极限后再做水平缩放。每个故障域你需要运行至少一个实例来提供容错机制。k8s节点不会自动将流量引向相同故障区域中的控制平面端点，但是您的云服务提供商可能有他自己的实现。例如，使用托管的负载均衡器，你可以配置负载均衡器发送源自故障域A中的kubelet和pod的流量，到也位于区域A中的控制平面主机。如果单节点的控制平面发生故障，导致区域A离线，则会导致区域A中节点的所有控制平面流量现在都在区域之间发送。在每个域中运行多个控制平面可以减少这种情况产生。

## etcd存储

为了提高大型集群的性能，您可以通过将事件信息独立存储到etcd实例中。当创建集群的时候，你可以：
* 启用并配置额外的etcd实例
* 配置apiserver使用etcd存储事件信息

## 插件资源

k8s资源限制特性使得内存泄漏和来自pod或者容器的影响最小的影响到其他组件。这些资源限制可以并且应该作用于插件上，正如它们作用在工作负载上一样。例如，你可以为一个日志组件设置CPU和内存限制：
```code
 ...
  containers:
  - name: fluentd-cloud-logging
    image: fluent/fluentd-kubernetes-daemonset:v1
    resources:
      limits:
        cpu: 100m
        memory: 200Mi
```

 插件的默认限制一般基于实践中中小型集群运行插件的经验数据。当运行大型集群的时候，插件经常会消耗比默认限额多很多的资源。如果一个大型集群部署的时候没有调整这些参数，这些插件会不停的由于达到内存限额而被杀死，同理，这些插件也许会由于CPU时间片的限额而运行在低性能下。
 为了避免遇到插件的资源限制问题，当创建一个多节点的集群的时候，您需要考虑以下几点：
 * 一些插件垂直扩展-那些一个故障域只需要单个副本的插件；对于这些插件，请在扩展集群的时候增加资源和限制。
 * 大多数插件水平扩展-那些可以通过运行多个pod提升容量的插件；但是针对一个超大的几集群，你可能也需要同时轻微的增加CPU和内存限制。 `VerticalPodAutoscaler` 运行在`recommender`模式下可以为请求与限制提供数额建议。
 * 一些插件需要一个节点运行一个-那些通过`DaemonSet`部署的插件；例如，一个节点级别的日志聚合器，类似于需要水平扩展的插件，这些插件也需要轻微的增加CPU和内存限制。

 ## 其他

`VerticalPodAutoscaler`是一个自定义资源，你可以部署到集群汇总帮助管理pod的资源请求与限制。

`cluster autoscaler`通过与一系列的云服务提供者交互，帮助你的集群运行足够数量的节点。
