{"./":{"url":"./","title":"本书介绍","keywords":"","body":"kubernetes翻译文档 包含以下内容： k8s架构分析 k8s插件介绍 部署实践教程 注解二次开发 "},"setup/":{"url":"setup/","title":"起步文档","keywords":"","body":"起步文档 本章节会列出几种不同的部署并运行k8s集群的方案。当您部署k8s集群的时，请参考以下几个方面作出选择：维护成本、安全性、控制难易、可用资源以及专业知识。 您将k8s集群部署到本地机器、云厂商、数据中心或者选择一个已经部署好的集群。当然，自定义的方案还有云服务提供商和裸机原生环境等。 学习环境 如果您正在学习k8s，建议使用k8s社区支持的工具，或者使用k8s生态中的工具在本地部署一套集群。 生产环境 当应用到生产环境中的时候，请考虑自己的操作技能掌握，或者托管给第三方提供商。 "},"setup/release.html":{"url":"setup/release.html","title":"发版概要","keywords":"","body":"发版提示与版本概要 v1.20 Release Notes Kubernetes version and version skew support policy "},"tasks/tools.html":{"url":"tasks/tools.html","title":"学习环境","keywords":"","body":"安装工具 "},"setup/production-environment.html":{"url":"setup/production-environment.html","title":"生产环境","keywords":"","body":"生产环境介绍 Container runtimes Installing Kubernetes with deployment tools Turnkey Cloud Solutions Windows in Kubernetes "},"setup/best-practices/":{"url":"setup/best-practices/","title":"最佳实践","keywords":"","body":"最佳实践 Considerations for large clusters Running in multiple zones Validate node setup PKI certificates and requirements "},"setup/best-practices/cluster-large.html":{"url":"setup/best-practices/cluster-large.html","title":"考虑大型集群","keywords":"","body":"考虑大型集群 一个集群是由一系列运行k8s客户端的节点（物理机器或者虚拟机）组成，该集群受到控制平面的管理，k8s1.20版本支持高达5000个节点的集群。更具体地来说，k8s基于以下标准设计： 每个节点不超过100个容器组 集群不超过5000个节点 集群总共不超过150000个容器组 集群总共不抄错300000个容器 您可以通过增加或者移除节点实现集群的伸缩，能否达成该点取决于集群的部署方式。 云服务厂商的资源限制 为了避免遇到云服务提供上限额的问题，当常见多节点的集群时候，需要考虑以下几点： 为云资源请求增长的限额，例如：计算实例，CPU核数，存储卷，固定IP，数据包过滤规则，负载均衡器，子网段数量，日志流； 为增加新节点预留空间，因为一些云服务厂商会限制新建实例的速率。 控制平面组件 对于大型集群来说，你需要一个拥有强大算力和其他资源的控制平面。一般情况下，你需要为每一个故障域运行1个到2个控制平面。需要时，先对实例做垂直缩放，当达到极限后再做水平缩放。每个故障域你需要运行至少一个实例来提供容错机制。k8s节点不会自动将流量引向相同故障区域中的控制平面端点，但是您的云服务提供商可能有他自己的实现。例如，使用托管的负载均衡器，你可以配置负载均衡器发送源自故障域A中的kubelet和pod的流量，到也位于区域A中的控制平面主机。如果单节点的控制平面发生故障，导致区域A离线，则会导致区域A中节点的所有控制平面流量现在都在区域之间发送。在每个域中运行多个控制平面可以减少这种情况产生。 etcd存储 为了提高大型集群的性能，您可以通过将事件信息独立存储到etcd实例中。当创建集群的时候，你可以： 启用并配置额外的etcd实例 配置apiserver使用etcd存储事件信息 插件资源 k8s资源限制特性使得内存泄漏和来自pod或者容器的影响最小的影响到其他组件。这些资源限制可以并且应该作用于插件上，正如它们作用在工作负载上一样。例如，你可以为一个日志组件设置CPU和内存限制： ... containers: - name: fluentd-cloud-logging image: fluent/fluentd-kubernetes-daemonset:v1 resources: limits: cpu: 100m memory: 200Mi 插件的默认限制一般基于实践中中小型集群运行插件的经验数据。当运行大型集群的时候，插件经常会消耗比默认限额多很多的资源。如果一个大型集群部署的时候没有调整这些参数，这些插件会不停的由于达到内存限额而被杀死，同理，这些插件也许会由于CPU时间片的限额而运行在低性能下。 为了避免遇到插件的资源限制问题，当创建一个多节点的集群的时候，您需要考虑以下几点： 一些插件垂直扩展-那些一个故障域只需要单个副本的插件；对于这些插件，请在扩展集群的时候增加资源和限制。 大多数插件水平扩展-那些可以通过运行多个pod提升容量的插件；但是针对一个超大的几集群，你可能也需要同时轻微的增加CPU和内存限制。 VerticalPodAutoscaler 运行在recommender模式下可以为请求与限制提供数额建议。 一些插件需要一个节点运行一个-那些通过DaemonSet部署的插件；例如，一个节点级别的日志聚合器，类似于需要水平扩展的插件，这些插件也需要轻微的增加CPU和内存限制。 其他 VerticalPodAutoscaler是一个自定义资源，你可以部署到集群汇总帮助管理pod的资源请求与限制。 cluster autoscaler通过与一系列的云服务提供者交互，帮助你的集群运行足够数量的节点。 "},"setup/best-practices/multiple-zones.html":{"url":"setup/best-practices/multiple-zones.html","title":"多域环境运行","keywords":"","body":"多域环境运行 本节讲解在k8s中运行多个环境。 背景 经过设计，k8s集群可以应对多个故障域问题的情况，一般来说这些域对应到逻辑组中叫做region。主流的云平台定义了地域为一系列故障域的集合，并提供一系列的特色服务：同一地域中，每个故障域提供相同的接口和服务。 控制平面行为 所有的控制平面组件都支持作为可互换资源池运行，每个组件都可被复制。部署集群控制平面时，请将控制平面组件的副本放置在多个故障区域中。如果可用性是一个重要问题，请选择至少三个故障区域，并在至少三个故障区域中复制每个单独的控制平面组件（API服务器，调度程序等，群集控制器管理器）。如果您使用了云厂商提供的控制器，您也应当保证故障域有多个副本。 注意 Kubernetes不为API服务器端点提供跨区域弹性。 您可以使用各种技术来提高群集API服务器的可用性，包括DNS轮询，SRV记录或具有运行状况检查的第三方负载平衡解决方案。 节点行为 节点启动时，每个节点上的kubelet会自动的将标签添加到代表kubernetes api中该特定的kubelet的Node对象。这些标签信息可以包含域信息。如果你的集群跨多个域分布，建议将节点标签搭配pod扩展约束一起使用，以控制pod如何在整个故障域之间分布。这样操作可以使得调度器将pod调度到一个理想的可用性状态，减少相关的出错影响整个工作负载带来的风险。例如，条件允许时，你可以设置限制，将3个副本的StatefulSet应用部署到不同的域中。你可以生命性的定义它，而无需显示定义每个工作负载正在使用哪些域。 节点的跨域分布 k8s本身并不会为您创建节点，你需要自己来执行此操作，或者使用注入Cluster API之类的工具来管理你的节点。使用Cluster API之类的工具可以定义一系列机器集合，这些机器集合作为工作节点分布运行在多个故障域中，也会根据规则自愈集群。 管理域为pod的划分 您可以为你创建的pod设置节点标签限制器，同理，在工作负载类的资源的pod模板中也可以这样操作，例如Deployment,StatefulSet,Job。 存储的跨域 当持久化卷创建时，管理控制器会自动的为关联到域中的持久化卷增加PersistentVolumeLabel标签。调度器随后会做确认，通过NoVolumeZoneConflict来确认pod可以声明一个同域中的持久化卷。你可以为持久化卷声明指定一个存储控制器，该类指定该类中的存储可以使用的故障域。 网络 k8s本身并不支持无感网络，您可以使用网络插件来配置集群网络，并且该网络解决方案可能具有特定于区域的元素。例如，如果您的云服务提供商支持type=LoadBalancer的代理，则负载均衡器可能仅将流量发送到与处理同一连接的负载均衡器的同一个区域中的pod。关于这点，请查阅云厂商的文档。对于自定义或者本地部署的情况，也有类似的注意事项。Service和Ingress的行为，包括如何处理不同的故障与，很大程度上取决于您的集群的部署方式。 错误恢复 当部署集群的时候，你也许需要考虑下是否以及如何让你的集群可以在多个故障域同时离线的情况下恢复，例如，你依赖于每个故障域至少有一个运行pod的节点吗？要知道，任何集群成对的工作不会依赖于集群中至少一个健康节点。例如，如果所有节点都处于不健康状态，你也许需要运行一个修复任务，这个任务会需要一个特殊的容忍度，才能够实现修复至少一个节点的服务。针对这个问题，k8s并没有给出解答，但是这仍然是需要考量的。 "},"setup/best-practices/node-conformance.html":{"url":"setup/best-practices/node-conformance.html","title":"校验节点部署","keywords":"","body":"校验节点部署 节点一致性测试 节点一致性测试是一个容器化的测试框架，可为节点提供系统验证和功能测试。 该测试将验证该节点是否满足Kubernetes的最低要求； 通过测试的节点有资格加入Kubernetes集群。 节点先觉条件 要想运行节点一致性测试，节点必须满足于标准化k8s节点相同的先决条件。至少，该节点需要运行以下守护程序： Container Runtime Kubelet 运行节点一致性测试 运行节点一致性测试，请按照以下步骤操作： 确认kubelet的--kubeconfig值；比如，指定--kubeconfig=/var/lib/kubelet/config.yaml。原因在于测试框架使用本地控制平面来测试kubelet，使用http://localhost:8080作为APIServer的地址。其他可能用到的kubelet命令行参数如下： --pod-cidr: 如果使用了kubenet，需要为kubelet指定一个随机的CIDR，例如：--pod-cidr=10.180.0.0/24。 --cloud-provider：如果使用了参数--cloud-provider=gce，运行前需要移除该参数。 运行以下命令进行测试： # $CONFIG_DIR is the pod manifest path of your Kubelet. # $LOG_DIR is the test output path. sudo docker run -it --rm --privileged --net=host \\ -v /:/rootfs -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\ k8s.gcr.io/node-test:0.2 其他架构下的一致性测试 k8s为其他架构提供了一致性测试docker镜像： arch image amd64 node-test-amd64 arm node-test-arm arm64 node-test-arm64 运行部分测试 要运行特定的测试，请用要运行的测试的正则表达式覆盖环境变量FOCUS: sudo docker run -it --rm --privileged --net=host \\ -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\ -e FOCUS=MirrorPod \\ # Only run MirrorPod test k8s.gcr.io/node-test:0.2 要跳过特定的测试，请用要跳过的测试的正则表达式覆盖环境变量SKIP: sudo docker run -it --rm --privileged --net=host \\ -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\ -e SKIP=MirrorPod \\ # Run all conformance tests but skip MirrorPod test k8s.gcr.io/node-test:0.2 节点一致性测试是节点e2e测试的容器化版本。默认情况下，它将运行所有一致性测试。从理论上讲，如果您配置了容器并正确安装了所需的卷，则可以运行任何节点e2e测试。 但强烈建议仅运行一致性测试，因为它需要更复杂的配置才能运行不一致性测试。 注意事项 该项测试会在节点上留下docker镜像，包含一致性测试镜像和功能测试的容器 测试会在节点上留下停止运行的容器，这些容器在功能测试的过程中创建。 "},"setup/best-practices/certificates.html":{"url":"setup/best-practices/certificates.html","title":"PKI证书问题","keywords":"","body":"PKI证书问题 k8s需要基于TLS的PKI证书授权。如果您使用了kubeadm部署的k8s，集群会自动生成需要的证书。当然，你也可以自己手动创建证书。例如，为了保证私有证书更加安全，你可以将其不存放在API Server中。本文讲解集群需要的证书。 集群是如何使用证书的 k8s在以下操作的时候需要用到PKI认证： kubelet访问API Server需要客户端证书 APIserver 端点需要服务端证书 集群管理员认证API server需要客户端证书 API server与kubelet通信需要客户端证书 API server与etcd通信需要客户端证书 controller manager与API server通信时候需要客户端证书和kubeconfig scheduler与API server通信时候需要客户端证书和kubeconfig 前置代理需要客户端和服务端证书 注意 前置代理需要证书的场景仅存在于运行kube-proxy支撑APIserver扩展的时候。 etcd在授权客户端和节点的时候也实现了一般的TLS认证 证书的存放位置 使用kubeadm安装k8s的时候，证书存放在/etc/kubernetes/pki，本文中提到的所有路径都是相对于该目录。 手动配置证书 如果你不想kubeadm自动生成证书的话，你可以使用以下几种方式创建： 单独根CA证书 你可以使用管理员创建一个根证书。然后，该根CA可以创建多个中间CA，并将所有进一步的创建委托给Kubernetes本身。 需要的CA： 路径 默认cn 描述 ca.crt,key kubernetes-ca Kubernetes general CA etcd/ca.crt,key etcd-ca For all etcd-related functions front-proxy-ca.crt,key kubernetes-front-proxy-ca For the front-end proxy 在上述CA之上，还需要获取用于服务帐户管理的公共/私有密钥对sa.key和sa.pub。 所有证书 如果你不想复制这些证书到你的集群中，你可以自己生成这些证书。 需要的证书如下： 默认CN 父证书 O 类型 hosts kube-etcd etcd-ca server, client localhost, 127.0.0.1 kube-etcd-peer etcd-ca server, client hostname, Host_IP, localhost, 127.0.0.1 kube-etcd-healthcheck-client etcd-ca client kube-apiserver-etcd-client etcd-ca system:masters client kube-apiserver kubernetes-ca server hostname, Host_IP, advertise_IP, [1] kube-apiserver-kubelet-client kubernetes-ca system:masters client front-proxy-client kubernetes-front-proxy-ca client [1]： 表示访问集群的其他任何IP或者DNS名称，其中kind映射到一种或多种x509密钥用法类型： kind 密钥用法 server digital signature, key encipherment, server auth client digital signature, key encipherment, client auth 注意：上面列出的主机/SAN是获得工作群集的推荐主机； 如果特定设置需要，则可以在所有服务器证书上添加其他SAN。 注意：对于kubeadm用户而言只需要如下： 在不使用私钥的情况下复制到群集CA证书的方案在kubeadm文档中称为外部CA。 如果将上面的列表与kubeadm生成的PKI进行比较，请注意，如果使用外部etcd，则不会生成kube-etcd，kube-etcd-peer和kube-etcd-healthcheck-client证书。 证书路径 证书应当存放在推荐的路径下，路径需要通过特定的参数指定而不是路径。 Default CN recommended key path recommended cert path command key argument cert argument etcd-ca etcd/ca.key etcd/ca.crt kube-apiserver --etcd-cafile kube-apiserver-etcd-client apiserver-etcd-client.key apiserver-etcd-client.crt kube-apiserver --etcd-keyfile --etcd-certfile kubernetes-ca ca.key ca.crt kube-apiserver --client-ca-file kubernetes-ca ca.key ca.crt kube-controller-manager --cluster-signing-key-file --client-ca-file, --root-ca-file, --cluster-signing-cert-file kube-apiserver apiserver.key apiserver.crt kube-apiserver --tls-private-key-file --tls-cert-file kube-apiserver-kubelet-client apiserver-kubelet-client.key apiserver-kubelet-client.crt kube-apiserver --kubelet-client-key --kubelet-client-certificate front-proxy-ca front-proxy-ca.key front-proxy-ca.crt kube-apiserver --requestheader-client-ca-file front-proxy-ca front-proxy-ca.key front-proxy-ca.crt kube-controller-manager --requestheader-client-ca-file front-proxy-client front-proxy-client.key front-proxy-client.crt kube-apiserver --proxy-client-key-file --proxy-client-cert-file etcd-ca etcd/ca.key etcd/ca.crt etcd --trusted-ca-file, --peer-trusted-ca-file kube-etcd etcd/server.key etcd/server.crt etcd --key-file --cert-file kube-etcd-peer etcd/peer.key etcd/peer.crt etcd --peer-key-file --peer-cert-file etcd-ca etcd/ca.crt etcdctl --cacert kube-etcd-healthcheck-client etcd/healthcheck-client.key etcd/healthcheck-client.crt etcdctl --key --cert 相同的注意事项适用于服务帐户密钥对： private key path public key path command argument sa.key kube-controller-manager --service-account-private-key-file sa.pub kube-apiserver --service-account-key-file 用户账户的证书配置 你必须要配置这些管理员账户和服务账户： filename credential name Default CN O (in Subject) admin.conf default-admin kubernetes-admin system:masters kubelet.conf default-auth system:node: (see note) system:nodes controller-manager.conf default-controller-manager system:kube-controller-manager scheduler.conf default-scheduler system:kube-scheduler 注意：kubelet.conf的的值必须与kubelet向apiserver注册时提供的节点名称的值完全匹配。 有关更多详细信息，请阅读节点授权。 对于每个配置，请使用给定的CN和O生成一个x509证书/密钥对。 对每个配置运行kubectl，如下所示： KUBECONFIG= kubectl config set-cluster default-cluster --server=https://:6443 --certificate-authority --embed-certs KUBECONFIG= kubectl config set-credentials --client-key .pem --client-certificate .pem --embed-certs KUBECONFIG= kubectl config set-context default-system --cluster default-cluster --user KUBECONFIG= kubectl config use-context default-system 这些文件的用途如下： filename command comment admin.conf kubectl Configures administrator user for the cluster kubelet.conf kubelet One required for each node in the cluster. controller-manager.conf kube-controller-manager Must be added to manifest in manifests/kube-controller-manager.yaml scheduler.conf kube-scheduler Must be added to manifest in manifests/kube-scheduler.yaml "},"concepts/":{"url":"concepts/","title":"内容详解","keywords":"","body":"内容详解 本章节帮助你了解Kubernetes系统的各个部分以及Kubernetes用于表示集群的抽象，并有助于您更深入地了解Kubernetes的工作原理。 "},"concepts/overview/":{"url":"concepts/overview/","title":"集群概览","keywords":"","body":"集群概览 什么是k8s k8s是一个便携的，可扩展的，以及开源的平台，用来管理容器化的工作负载和服务，该基础设施兼具声明式配置和自动化特性。k8s是一个大型的，快速成长的生态系统。使用k8s服务，支持以及工具，具备广泛的可行性。 k8s组件 k8s集群由代表控制平面的组件以及机器集合组成的node节点组件组成。 k8s api k8s api允许你查询和管理集群对象的状态。k8s核心的控制平面包含由api server以及http api暴露出来的接口。用户之间，集群中的不同部分，以及外部组件之间的所有通信都是通过api server完成。 理解k8s对象 k8s对象是k8s系统中的持久化内容。k8s使用这些内容代表集群。学习k8s对象模型，以及如何使用这些对象。 "},"concepts/overview/what-is-kubernetes.html":{"url":"concepts/overview/what-is-kubernetes.html","title":"什么是k8s","keywords":"","body":"什么是k8s 当前页面是k8s的简介。 k8s是一个便携的，可扩展的，以及开源的平台，用来管理容器化的工作负载和服务，该基础设施兼具声明式配置和自动化特性。k8s是一个大型的，快速成长的生态系统。使用k8s服务，支持以及工具，具备广泛的可行性。 Kubernetes这个名字起源于希腊语，意思是舵手或飞行员。Google于2014年开源了kubernetes项目。Kubernetes将超过15年的Google在大规模生产工作负载方面的经验与社区中最好的想法和实践相结合。 回溯以往 想要理解kubernetes为什么这么有用处，让我们把时间往前推。 传统的部署：早期，企业一般在物理机上运行应用。在物理机上没有办法解决资源的依赖，由此导致了资源的分配问题。例如，如果多个应用运行在同一个物理机上，某些实例可能会出现单个应用占用几乎全部资源的情况，带来的结果就是，其他的应用表现不佳。这种现象的解决方案之一是应用运行在不同的物理机上。但是，这样的话当资源利用不足的时候无法缩小资源，同时企业管理很多的物理机器是非常昂贵的。 *虚拟化部署：虚拟化部署作为一个解决方案被提了出来。该方案允许在单个物理服务器上运行多个虚拟的机器。虚拟化方案将应用隔离在了虚拟机层面，并且提供了一定程度的安全性，因为一个应用的信息无法随意的被另外一个应用访问。 虚拟化方案提供了比物理机器更好的资源利用率，以及更好的可扩展性，因为一个应用可以很容易的新增和更新，减少了硬件的开销以及其他开销。通过虚拟化方案，你可以设置一组物理资源作为虚拟机集群。 每一个虚拟机都是一个运行所有组件的机器，包含虚拟化的硬件以及之上的操作系统组件， 容器部署：容器话和虚拟机很类似，但是它们可以更加轻松的在应用间隔离操作系统。因此，容器被认为是更加轻量级的。类似于虚拟机，一个容器有其自身的文件系统，共享的cpu，共享的内存，进程空间，以及其他。因为它们与基础架构的分离，所以可以跨云和OS进行分发。 容器化变得流行，是因为它们提供了额外的好处，例如： 敏捷的应用创建与部署：相比于虚拟机镜像的使用，容器化提升了效率 持续开发，持续继承，以及持续部署：通过快速有效的回滚提供可靠且频繁的容器映像构建和部署 开发和运维的关注点分离：在构建/发版的时候创建应用镜像，而不是开发的时候，由此解偶了应用月基础架构 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他信号。 跨开发，测试和生产的环境一致性：在笔记本电脑上与在云中一样在笔记本电脑上运行。 云和操作系统分发的可移植性：可在Ubuntu，RHEL，CoreOS，本地，主要公共云以及其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行操作系统到使用逻辑资源在操作系统上运行应用程序。 松散耦合，分布式，弹性，解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理–而不是在一台大型单机上运行的单片堆栈。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。 为什么你需要kubernetes以及它可以做什么 容器化是一个很好的方式去绑定并运行应用。在一个生产环境中，你需要管理运行应用的容器并且确保没有宕机时间。例如：如果一个容器宕机，另外一个容器需要启动。这种行为如果有系统来掌控是不是变得简单的多了？ 这就是k8s自愈的原理!k8s为你提供了一个弹性的分布式系统作为一个基础组件。它关注为你的应用扩容和故障转移，提供了部署模式以及更多。例如：k8s可以很容易的为你的系统提供金丝雀发布。 kubernetes为你提供如下： 服务发现与负载均衡：k8s通过DNS名称或者它们自身的IP地址暴露容器。如果同一容器流量过高，kubernete能够均衡负载并且分发网络流量，保证部署服务的稳定性。 存储编排：Kubernetes允许您自动挂载您选择的存储系统，例如本地存储，公共云提供商等。 自动部署和回滚：您可以使用Kubernetes描述已部署容器的所需状态，它可以以受控的速率将实际状态更改为所需状态。 例如，您可以自动化Kubernetes来为您的部署创建新容器，删除现有容器并将其所有资源用于新容器。 自动资源回收：您为Kubernetes提供了一个节点集群，可用于运行容器化任务。 您告诉Kubernetes每个容器需要多少CPU和内存（RAM）。 Kubernetes可以将容器安装到您的节点上，以充分利用您的资源。 自愈：Kubernetes重新启动失败的容器，替换容器，杀死不响应用户定义的运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 密钥与配置管理：Kubernetes允许您存储和管理敏感信息，例如密码，OAuth令牌和SSH密钥。 您可以部署和更新机密信息和应用程序配置，而无需重建容器映像，也无需在堆栈配置中公开机密信息。 k8s不能做到什么 Kubernetes不是传统的，包罗万象的PaaS（平台即服务）系统。 由于Kubernetes在容器级别而非硬件级别运行，因此它提供了PaaS产品共有的一些普遍适用的功能，例如部署，扩展，负载平衡，并允许用户集成其日志记录，监视和警报解决方案。 但是，Kubernetes并不是单片的，并且这些默认解决方案是可选的和可插入的。 Kubernetes提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。 k8s: 不限制支持的应用程序类型。 Kubernetes旨在支持极为多样化的工作负载，包括无状态，有状态和数据处理工作负载。 如果应用程序可以在容器中运行，那么它应该可以在Kubernetes上很好地运行。 不部署源代码，也不构建您的应用程序。 持续集成，交付和部署（CI / CD）工作流取决于组织的文化和偏好以及技术要求。 不提供应用程序级别的服务，例如中间件（例如，消息总线），数据处理框架（例如，Spark），数据库（例如，MySQL），缓存或群集存储系统（例如，Ceph） 作为内置服务。 这样的组件可以在Kubernetes上运行，并且/或者可以由Kubernetes上运行的应用程序通过诸如开放服务代理之类的可移植机制来访问。 不指示日志记录，监视或警报解决方案。 它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。 不提供也不要求配置语言/系统（例如，Jsonnet）。 它提供了一个声明性API，可以通过任意形式的声明性规范作为目标。 不提供也不采用任何全面的机器配置，维护，管理或自我修复系统。 此外，Kubernetes不仅仅是一个编排系统。 实际上，它消除了编排的需要。 编排的技术定义是执行定义的工作流程：首先执行A，然后执行B，然后执行C。相反，Kubernetes包含一组独立的，可组合的控制过程，这些过程连续地将当前状态驱动到提供的所需状态。 从A到C的方式无关紧要。集中控制也不是必需的。 这使得系统更易于使用，功能更强大，更健壮，更具弹性和可扩展性。 "},"concepts/overview/components.html":{"url":"concepts/overview/components.html","title":"k8s组件","keywords":"","body":"k8s组件 当你部署kubernetes的时候，你得到的是一个集群。 一个kubernetes集群由一系列叫做工作机器的节点组成，在这些节点上运行容器化的应用。每个集群拥有至少一个工作节点。 工作节点托管pod，这些pod是应用程序工作负载的组成部分。控制平面负责管理工作节点，以及集群中的pod。生产环境下，控制平面经常运行在多个机器上，同时一个集群通常运行多个节点，这提供了容错性与高可用。 本文档概述了拥有完整且有效的Kubernetes集群所需的各种组件。 以下是Kubernetes集群的示意图，其中所有组件都捆绑在一起。 控制平面组件 控制平面的组件对集群作出全局性的决策，例如调度，同时可以监测并相应集群事件；例如，当deployment的副本数量不满足的时候，启动一个新的pod。 控制平面的组件可以运行在集群中的任一台机器上。但是，为简单起见，设置脚本通常在同一台计算机上启动所有控制平面组件，并且不在该计算机上运行用户容器。有关多主虚拟机设置示例，请参阅构建高可用性群集。 apiserver api server是Kubernetes控制平面的组件，该组件公开Kubernetes API。 api server是Kubernetes控制平面的前端。 Kubernetes API server的主要实现是kube-apiserver。 kube-apiserver旨在水平扩展-即，它通过部署更多实例进行扩展。 您可以运行kube-apiserver的多个实例，并平衡这些实例之间的流量。 etcd 一致且高度可用的键值存储用作所有集群数据的Kubernetes的后备存储。 如果您的Kubernetes集群使用etcd作为其后备存储，请确保您有针对这些数据的备份计划。 您可以在官方文档中找到有关etcd的详细信息。 kube-scheduler 控制平面组件，该组件监视没有分配节点的新创建的Pod，并选择一个节点以在其上运行。 计划决策要考虑的因素包括：个体和集体资源需求，硬件/软件/策略约束，亲和力和反亲和力规范，数据局部性，工作负载之间的干扰以及期限。 kube-controller-manager 运行控制器进程的控制平面组件。 从逻辑上讲，每个控制器是一个单独的进程，但是为了降低复杂性，它们都被编译为单个二进制文件并在单个进程中运行。 这些控制器的类型包含： 节点控制器：负责在节点出现故障时进行通知和响应。 作业控制器：监视代表一次性任务的作业对象，然后创建Pod以运行这些任务以完成任务。 端点控制器：填充“端点”对象（即，加入“服务和窗格”）。 服务帐户和令牌控制器：为新的名称空间创建默认帐户和API访问令牌。 cloud-controller-manager 嵌入了特定于云的控制逻辑的Kubernetes控制平面组件。 云控制器管理器使您可以将集群链接到云提供商的API，并将与该云平台交互的组件与仅与集群交互的组件分开。 cloud-controller-manager仅运行特定于您的云提供商的控制器。 如果您是在自己的场所或PC内的学习环境中运行Kubernetes，则该群集没有云控制器管理器。 与kube-controller-manager一样，cloud-controller-manager将多个逻辑上独立的控制循环组合为一个二进制文件，您可以将其作为单个进程运行。 您可以水平缩放（运行多个副本）以提高性能或帮助容忍故障。 以下控制器可以具有云提供程序依赖性： 节点控制器：用于检查云提供程序以确定节点停止响应后是否已在云中删除该节点 路由控制器：用于在基础云基础架构中设置路由 服务控制器：用于创建，更新和删除云提供商负载平衡器 工作节点组件 节点组件在每个节点上运行，维护运行中的Pod，并提供Kubernetes运行时环境。 kubelet 在集群中每个节点上运行的代理。 确保容器在Pod中运行。 kubelet包含通过各种机制提供的一组PodSpec，并确保这些PodSpec中描述的容器正在运行且状况良好。 Kubelet不管理不是Kubernetes创建的容器。 kube-proxy kube-proxy是一个网络代理，它在集群中的每个节点上运行，实现了Kubernetes Service概念的一部分。 kube-proxy维护节点上的网络规则。 这些网络规则允许从群集内部或外部的网络会话与Pod进行网络通信。 如果有kube-proxy可用，它将使用操作系统数据包过滤层。 否则，kube-proxy会转发流量本身。 Container runtime Container runtime是负责运行容器的软件。 Kubernetes支持几种容器运行时：Docker，容器化，CRI-O以及Kubernetes CRI（容器运行时接口）的任何实现。 插件 插件使用Kubernetes资源（DaemonSet，Deployment等）来实现集群功能。 由于这些功能提供集群级功能，因此插件的命名空间资源属于kube-system命名空间。 所选的插件说明如下： 有关可用插件的扩展列表，请参阅插件。 DNS 尽管并非严格要求其他附加组件，但由于许多示例都依赖于此，因此所有Kubernetes群集都应具有群集DNS。 除了您环境中的其他DNS服务器之外，群集DNS还是一个DNS服务器，它为Kubernetes服务提供DNS记录。 由Kubernetes启动的容器会在其DNS搜索中自动包括此DNS服务器。 仪表盘 仪表板是Kubernetes集群的基于Web的通用UI。 它允许用户管理集群中运行的应用程序以及集群本身并进行故障排除。 容器资源监控 容器资源监视在中央数据库中记录有关容器的一般时间序列指标，并提供用于浏览该数据的UI。 集群级别的日志 集群级别的日志记录机制负责通过搜索/浏览界面将容器日志保存到中央日志存储中。 "},"concepts/overview/kubernetes-api.html":{"url":"concepts/overview/kubernetes-api.html","title":"k8s api","keywords":"","body":"k8s api kubernetes控制平面的核心是api server。api server暴露出HTTP API，这保证了终端用户、集群间的不同部分、以及外部组件，之间互相通信。 Kubernetes API使您可以查询和操纵Kubernetes中API对象的状态（例如：Pods，命名空间，ConfigMap和Events）。 多数操作可以通过kubectl命令行界面或其他命令行工具（例如kubeadm）执行，这些工具依次使用API。 但是，您也可以使用REST调用直接访问API。 如果要使用Kubernetes API编写应用程序，请考虑使用其中一种客户端库。 OpenAPI 特性 使用OpenAPI记录了完整的API详细信息。 Kubernetes API服务器通过/ openapi / v2端点提供OpenAPI规范。 您可以使用请求标头来请求响应格式，如下所示： Header Possible values Notes Accept-Encoding gzip not supplying this header is also acceptable Accept application/com.github.proto-openapi.spec.v2@v1.0+protobuf mainly for intra-cluster use application/json default * serves application/json Kubernetes实现了另一种基于Protobuf的序列化格式，该格式主要用于集群内通信。 有关此格式的更多信息，请参阅Kubernetes Protobuf序列化设计建议以及位于定义API对象的Go包中的每个架构的接口定义语言（IDL）文件。 持久化 Kubernetes通过将对象的序列化状态写入etcd来存储它们。 API 分组与版本 为了更轻松地消除字段或重组资源表示形式，Kubernetes支持多个API版本，每个版本位于不同的API路径，例如/ api / v1或/apis/rbac.authorization.k8s.io/v1alpha1。 版本控制是在API级别而不是资源或字段级别完成的，以确保API呈现系统资源和行为的清晰一致的视图，并能够控制对寿命终止和/或实验性API的访问。 为了使其易于开发和扩展其API，Kubernetes实现了可以启用或禁用的API组。 API资源通过其API组，资源类型，名称空间（用于命名空间的资源）和名称来区分。 API服务器透明地处理API版本之间的转换：所有不同版本实际上是相同持久数据的表示。 API服务器可以通过多个API版本提供相同的基础数据。 例如，假设对于同一资源有两个API版本，即v1和v1beta1。 如果最初使用其API的v1beta1版本创建了对象，则以后可以使用v1beta1或v1 API版本读取，更新或删除该对象。 API变动 任何成功的系统都需要随着新用例的出现或现有用例的变化而增长和变化。 因此，Kubernetes设计了Kubernetes API来不断变化和增长。 Kubernetes项目旨在不破坏与现有客户端的兼容性，并在一段时间内保持这种兼容性，以便其他项目有机会进行调整。 通常，可以频繁地添加新的API资源和新的资源字段。 消除资源或字段要求遵循API弃用政策。 一旦正式的Kubernetes API达到通用版本（GA）（通常在API版本v1），Kubernetes便会保持其兼容性。 此外，即使在可行的情况下，Kubernetes仍可与Beta API版本保持兼容性：如果您采用Beta API，即使功能稳定后，您仍可以继续使用该API与集群进行交互。 注意：尽管Kubernetes还旨在保持与Alpha API版本的兼容性，但是在某些情况下这是不可能的。 如果您使用任何Alpha API版本，请在升级集群时查看Kubernetes的发行说明，以防API确实发生了变化。 API扩展 Kubernetes API可以通过以下两种方式之一进行扩展： 使用自定义资源，您可以声明性地定义API服务器应如何提供所选资源API。 您还可以通过实现聚合层来扩展Kubernetes API。 "},"concepts/overview/working-with-objects/":{"url":"concepts/overview/working-with-objects/","title":"k8s 资源对象","keywords":"","body":"k8s 资源对象 理解kubernetes对象 kubernetes对象管理 对象名称和ID 名称空间 标签与选择器 注解 字段选择器 推荐的字段 "},"concepts/overview/working-with-objects/kubernetes-objects.html":{"url":"concepts/overview/working-with-objects/kubernetes-objects.html","title":"理解k8s对象","keywords":"","body":"理解k8s对象 本文介绍了如何在Kubernetes API中表示Kubernetes对象，以及如何以.yaml格式表示它们。 理解k8s对象 Kubernetes对象是Kubernetes系统中的持久实体。 Kubernetes使用这些实体来表示集群的状态。 具体来说，它们可以描述： 哪些容器化的应用程序正在运行（以及在哪些节点上） 这些应用程序可用的资源 有关这些应用程序的行为的策略，例如重新启动策略，升级和容错 Kubernetes对象是“意图记录”-创建对象后，Kubernetes系统将不断工作以确保该对象存在。 通过创建一个对象，您可以有效地告诉Kubernetes系统您希望集群的工作负荷是什么样子。 这是您的集群的期望状态。 要使用Kubernetes对象-无论是创建，修改还是删除它们-您都需要使用Kubernetes API。 例如，当您使用kubectl命令行界面时，CLI会为您进行必要的Kubernetes API调用。 您还可以使用客户端库之一在自己的程序中直接使用Kubernetes API。 对象特性和状态 几乎每个Kubernetes对象都包含两个嵌套的对象字段，这些字段控制对象的配置：对象规范和对象状态。 对于具有规格的对象，必须在创建对象时进行设置，并提供所需资源的特征描述：所需状态。 状态描述了对象的当前状态，该状态由Kubernetes系统及其组件提供和更新。 Kubernetes控制平面连续不断地主动管理每个对象的实际状态，以匹配您提供的所需状态。 例如：在Kubernetes中，Deployment是一个对象，可以表示您在集群上运行的应用程序。 创建展开时，可以将展开规范设置为指定要运行该应用程序的三个副本。 Kubernetes系统读取Deployment规范并启动所需应用程序的三个实例-更新状态以符合您的规范。 如果这些实例中的任何一个都应该失败（状态更改），则Kubernetes系统将通过进行更正来响应规范和状态之间的差异-在这种情况下，将启动替换实例。 有关对象规范，状态和元数据的更多信息，请参见Kubernetes API约定。 描述kubernetes对象 在Kubernetes中创建对象时，必须提供描述其所需状态的对象规范以及有关该对象的一些基本信息（例如名称）。 当您使用Kubernetes API创建对象（直接或通过kubectl）时，该API请求必须在请求正文中包含该信息作为JSON。 通常，您会在.yaml文件中将信息提供给kubectl。 发出API请求时，kubectl会将信息转换为JSON。 这是一个示例.yaml文件，显示了Kubernetes部署的必填字段和对象规范： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 使用.yaml文件创建展开的一种方法是使用上述方法，是在kubectl命令行界面中使用kubectl apply命令，并将.yaml文件作为参数传递。 这是一个例子： kubectl apply -f https://k8s.io/examples/application/deployment.yaml --record 输出类似如下： deployment.apps/nginx-deployment created 必备字段 在您要创建的Kubernetes对象的.yaml文件中，您需要为以下字段设置值： apiVersion：您正在使用哪个版本的Kubernetes API创建该对象 kind：你想要创建那种类型的对象 metadata：有助于唯一表示对象的数据，例如name,UID,namespace spec：你定义的对象期望状态 每个Kubernetes对象的对象规范的精确格式都不同，并且包含特定于该对象的嵌套字段。 Kubernetes API参考可以帮助您找到可以使用Kubernetes创建的所有对象的规范格式。 例如，可以在PodSpec v1核心中找到Pod的规范格式，而可以在DeploymentSpec v1应用中找到Deployment的规范格式。 "},"concepts/overview/working-with-objects/object-management.html":{"url":"concepts/overview/working-with-objects/object-management.html","title":"对象的管理","keywords":"","body":"对象的管理 kubectl命令行工具支持几种不同的方式来创建和管理Kubernetes对象。 本文档概述了不同的方法。 阅读Kubectl书，了解Kubectl管理对象的详细信息。 管理技巧 警告：应该仅使用一种技术来管理Kubernetes对象。 同一对象的混合和匹配技术会导致不确定的行为。 管理技巧 操作对象 推荐环境 支持的作者 学习曲线 命令式命令 实时对象 开发项目 多于1种 最低 命令式对象配置 单独文件 生产环境 1种 中等 声明式对象配置 文件夹 生产环境 多余1种 最高 命令式命令 使用命令式命令时，用户可以直接在集群中的活动对象上进行操作。 用户将对kubectl命令的操作作为参数或标志提供给用户。 这是在集群中开始或运行一次性任务的推荐方法。 因为此技术直接在活动对象上运行，所以它不提供先前配置的历史记录。 示例 通过创建Deployment对象来运行nginx容器的实例： kubectl create deployment nginx --image nginx 权衡取舍 与命令式对象配置相比的优势： 命令被表示为单个动作词。 命令仅需一步即可对集群进行更改。 与对象配置相比的缺点： 命令不与变更审查流程集成。 命令不提供与更改关联的审核跟踪。 除实时内容外，命令不提供记录源。 命令不提供用于创建新对象的模板。 命令式对象配置 在命令性对象配置中，kubectl命令指定操作（创建，替换等），可选标志和至少一个文件名。 指定的文件必须包含YAML或JSON格式的对象的完整定义。 警告：命令替换命令将现有规范替换为新提供的规范，并删除对配置文件中缺少的对象的所有更改。 此方法不应与规格独立于配置文件进行更新的资源类型一起使用。 例如，LoadBalancer类型的服务的externalIPs字段独立于集群的配置进行更新。 示例 创建定义在配置文件中的对象： kubectl create -f nginx.yaml 删除定义在2个配置文件中的对象： kubectl delete -f nginx.yaml -f redis.yaml 通过覆盖配置的方式更新定义在配置文件中的对象： kubectl replace -f nginx.yaml 权衡比较 相比命令交互式的优点如下： 对象的配置可以存放在源码控制系统例如git中 对象配置可以与流程集成，例如在推送和审计跟踪之前检查更改。 对象配置提供了用于创建新对象的模板。 相比于命令交互式的缺陷如下： 对象配置需要对对象架构有基本的了解。 对象配置需要额外的写入yaml文件的步骤。 相比于声明式配置的优点： 命令式对象配置行为更简单易懂。 从Kubernetes 1.5版开始，命令式对象配置更加成熟。 相比于声明式配置的缺点： 命令性对象配置最适合文件而不是目录。 对活动对象的更新必须反映在配置文件中，否则在下一次替换期间将丢失。 声明式配置 使用声明性对象配置时，用户对本地存储的对象配置文件进行操作，但是，用户未定义要对该文件执行的操作。 创建，更新和删除操作由kubectl自动检测到每个对象。 这样可以处理目录，其中不同的对象可能需要不同的操作。 注意：声明式对象配置保留其他编写者所做的更改，即使这些更改未合并回到对象配置文件中也是如此。 这可以通过使用补丁API操作仅写入观察到的差异，而不是使用replace API操作来替换整个对象配置来实现。 示例 处理configs目录中的所有对象配置文件，并创建或修补活动对象。 您可以先进行比较以查看将要进行的更改，然后应用： kubectl diff -f configs/ kubectl apply -f configs/ 递归处理目录： kubectl diff -R -f configs/ kubectl apply -R -f configs/ 权衡比较 相对于命令式对象配置的优点： 即使没有将它们直接合并回配置文件，也将保留直接对活动对象所做的更改。 声明性对象配置更好地支持对目录进行操作并自动检测每个对象的操作类型（创建，修补，删除）。 相对于命令式对象配置的缺点： 声明性对象配置在意外发生时更难调试和理解结果。 使用差异的部分更新会创建复杂的合并和补丁操作。 "},"concepts/overview/working-with-objects/names.html":{"url":"concepts/overview/working-with-objects/names.html","title":"对象的名称和ID","keywords":"","body":"对象的名称和ID 集群中的每个对象都有其该类型的唯一名称。每一个kubernetes对象都有一个在整个集群唯一的UUID。 名称 一个客户端提供的字符串类型，指向该对象的资源链接，例如/api/v1/pods/some-name。 同一时间，只有给定的类型的对象可以拥有一个给定的名称。但是，如果你删除了这个对象，你可以重新创建同名的对象。 以下是3种命名资源的常用方式。 DNS子域名称 大多数资源类型必须包含可以用作DNS子域名的名字，符合RFC 1123标准。这意味着这个名字必须包含： 不多余253个字符 只包含消协字母数字，以及-,. 字母数字开头 字母数字结束 DNS标签名称 某些资源类型要求其名称遵循RFC 1123中定义的DNS标签标准。这意味着名称必须： 最多63个字符 只包含消协字母数字，以及-,. 字母数字开头 字母数字结束 路径部分名称 某些资源类型要求其名称能够被安全地编码为路径段。 换句话说，名称不能为“。” 或“ ..”，并且名称中不能包含“ /”或“％”。 以下是一个名为nginx-demo的Pod的示例清单： apiVersion: v1 kind: Pod metadata: name: nginx-demo spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 注意： 某些资源类型对其名称有其他限制。 UIDs Kubernetes系统生成的字符串，用于唯一标识对象。 在Kubernetes集群的整个生命周期中创建的每个对象都有一个独特的UID。 旨在区分相似实体的历史发生。 Kubernetes UID是普遍唯一的标识符（也称为UUID）。 UUID被标准化为ISO / IEC 9834-8和ITU-T X.667。 "},"concepts/overview/working-with-objects/namespaces.html":{"url":"concepts/overview/working-with-objects/namespaces.html","title":"名称空间","keywords":"","body":"名称空间 Kubernetes支持由同一物理群集支持的多个虚拟群集。 这些虚拟集群称为名称空间。 何时使用多个名称空间 命名空间旨在用于具有多个用户的环境，这些用户分布在多个团队或项目中。 对于拥有几到几十个用户的集群，您根本不需要创建或考虑名称空间。 当需要名称空间提供的功能时，请开始使用它们。 命名空间为名称提供了作用域。 资源名称在名称空间中必须唯一，但在名称空间之间则必须唯一。 命名空间不能彼此嵌套，并且每个Kubernetes资源只能位于一个命名空间中。 命名空间是一种在多个用户之间（通过资源配额）划分群集资源的方法。 不必仅使用多个名称空间来分隔稍有不同的资源，例如同一软件的不同版本：使用标签来区分同一名称空间中的资源。 开始使用名称空间 名称空间管理指南中描述了名称空间的创建和删除。 注意：避免使用前缀kube-创建名称空间，因为它是为Kubernetes系统名称空间保留的。 查看 通过以下指令获取集群中的所有namespaces： kubectl get namespace NAME STATUS AGE default Active 1d kube-node-lease Active 1d kube-public Active 1d kube-system Active 1d "},"concepts/overview/working-with-objects/labels.html":{"url":"concepts/overview/working-with-objects/labels.html","title":"标签和标签选择器","keywords":"","body":"标签和标签选择器 "},"concepts/overview/working-with-objects/annotations.html":{"url":"concepts/overview/working-with-objects/annotations.html","title":"注解","keywords":"","body":"注解 "},"concepts/overview/working-with-objects/field-selectors.html":{"url":"concepts/overview/working-with-objects/field-selectors.html","title":"字段选择器","keywords":"","body":"字段选择器 "},"concepts/overview/working-with-objects/common-labels.html":{"url":"concepts/overview/working-with-objects/common-labels.html","title":"推荐的标签","keywords":"","body":"推荐的标签 "},"concepts/architecture/":{"url":"concepts/architecture/","title":"集群架构","keywords":"","body":"集群架构 "},"concepts/architecture/nodes.html":{"url":"concepts/architecture/nodes.html","title":"节点","keywords":"","body":"节点 "},"concepts/architecture/control-plane-node-communication.html":{"url":"concepts/architecture/control-plane-node-communication.html","title":"控制平面与节点通信","keywords":"","body":"控制平面与节点通信 "},"concepts/architecture/controller.html":{"url":"concepts/architecture/controller.html","title":"控制器","keywords":"","body":"控制器 "},"concepts/architecture/cloud-controller.html":{"url":"concepts/architecture/cloud-controller.html","title":"云控制器管理","keywords":"","body":"云控制器管理 "},"concepts/containers/":{"url":"concepts/containers/","title":"容器详解","keywords":"","body":"容器详解 "},"concepts/containers/images.html":{"url":"concepts/containers/images.html","title":"镜像","keywords":"","body":"镜像 "},"concepts/containers/container-environment.html":{"url":"concepts/containers/container-environment.html","title":"容器运行环境","keywords":"","body":"容器运行环境 "},"concepts/containers/runtime-class.html":{"url":"concepts/containers/runtime-class.html","title":"运行时类","keywords":"","body":"运行时类 "},"concepts/containers/container-lifecycle-hooks.html":{"url":"concepts/containers/container-lifecycle-hooks.html","title":"容器声明周期钩子","keywords":"","body":"容器声明周期钩子 "},"concepts/workloads/":{"url":"concepts/workloads/","title":"工作负载","keywords":"","body":"工作负载 "},"concepts/workloads/pods/":{"url":"concepts/workloads/pods/","title":"容器组","keywords":"","body":"容器组 "},"concepts/workloads/pods/pod-lifecycle.html":{"url":"concepts/workloads/pods/pod-lifecycle.html","title":"容器组声明周期","keywords":"","body":"容器组声明周期 "},"concepts/workloads/pods/init-containers.html":{"url":"concepts/workloads/pods/init-containers.html","title":"初始化容器","keywords":"","body":"初始化容器 "},"concepts/workloads/controllers/":{"url":"concepts/workloads/controllers/","title":"负载资源","keywords":"","body":"负载资源 "},"concepts/workloads/controllers/deployment.html":{"url":"concepts/workloads/controllers/deployment.html","title":"部署器","keywords":"","body":"部署器 "},"concepts/workloads/controllers/replicaset.html":{"url":"concepts/workloads/controllers/replicaset.html","title":"副本集","keywords":"","body":"副本集 "},"concepts/services-networking.html":{"url":"concepts/services-networking.html","title":"代理、负载均衡与网络","keywords":"","body":"代理、负载均衡与网络 "},"concepts/storage.html":{"url":"concepts/storage.html","title":"后端存储","keywords":"","body":"后端存储 "},"concepts/configuration.html":{"url":"concepts/configuration.html","title":"配置定义","keywords":"","body":"配置定义 "},"concepts/security.html":{"url":"concepts/security.html","title":"安全策略","keywords":"","body":"安全策略 "},"concepts/policy.html":{"url":"concepts/policy.html","title":"资源策略","keywords":"","body":"资源策略 "},"concepts/scheduling-eviction.html":{"url":"concepts/scheduling-eviction.html","title":"调度驱逐","keywords":"","body":"调度驱逐 "},"concepts/cluster-administration.html":{"url":"concepts/cluster-administration.html","title":"集群管理","keywords":"","body":"集群管理 "},"concepts/extend-kubernetes.html":{"url":"concepts/extend-kubernetes.html","title":"功能扩展","keywords":"","body":"功能扩展 "},"tasks/":{"url":"tasks/","title":"新手教程","keywords":"","body":"新手教程 "},"tasks/administer-cluste.html":{"url":"tasks/administer-cluste.html","title":"管理集群","keywords":"","body":"管理集群 "},"tasks/configure-pod-container.html":{"url":"tasks/configure-pod-container.html","title":"配置容器","keywords":"","body":"配置容器 "},"tasks/manage-kubernetes-objects.html":{"url":"tasks/manage-kubernetes-objects.html","title":"管理对象","keywords":"","body":"管理对象 "},"tasks/configmap-secret.html":{"url":"tasks/configmap-secret.html","title":"管理密钥","keywords":"","body":"管理密钥 "},"tasks/inject-data-application.html":{"url":"tasks/inject-data-application.html","title":"容器交互","keywords":"","body":"容器交互 "},"tasks/run-application.html":{"url":"tasks/run-application.html","title":"部署服务","keywords":"","body":"部署服务 "},"tasks/job.html":{"url":"tasks/job.html","title":"部署任务","keywords":"","body":"部署任务 "},"tasks/access-application-cluster.html":{"url":"tasks/access-application-cluster.html","title":"访问服务","keywords":"","body":"访问服务 "},"tasks/debug-application-cluster.html":{"url":"tasks/debug-application-cluster.html","title":"监控、审计与调试","keywords":"","body":"监控、审计与调试 "},"tasks/extend-kubernetes.html":{"url":"tasks/extend-kubernetes.html","title":"扩展集群","keywords":"","body":"扩展集群 "},"tasks/tls.html":{"url":"tasks/tls.html","title":"证书加密","keywords":"","body":"证书加密 "},"tasks/manage-daemon.html":{"url":"tasks/manage-daemon.html","title":"守护服务","keywords":"","body":"守护服务 "},"tasks/service-catalog.html":{"url":"tasks/service-catalog.html","title":"服务管理","keywords":"","body":"服务管理 "},"tasks/network.html":{"url":"tasks/network.html","title":"网络实践","keywords":"","body":"网络实践 "},"tasks/kubelet-credential-provider/kubelet-credential-provider.html":{"url":"tasks/kubelet-credential-provider/kubelet-credential-provider.html","title":"配置一个客户端景象认证服务","keywords":"","body":"配置一个客户端景象认证服务 "},"tasks/extend-kubectl/kubectl-plugins.html":{"url":"tasks/extend-kubectl/kubectl-plugins.html","title":"使用插件扩展集群","keywords":"","body":"使用插件扩展集群 "},"tasks/manage-hugepages/scheduling-hugepages.html":{"url":"tasks/manage-hugepages/scheduling-hugepages.html","title":"管理大页码","keywords":"","body":"管理大页码 "},"tasks/manage-gpus/scheduling-gpus.html":{"url":"tasks/manage-gpus/scheduling-gpus.html","title":"调度GPU","keywords":"","body":"调度GPU "}}