{"./":{"url":"./","title":"本书介绍","keywords":"","body":"kubernetes翻译文档 包含以下内容： k8s文档翻译 k8s架构分析 k8s插件介绍 部署实践教程 注解二次开发 操作实践示例 cka笔记 "},"setup/":{"url":"setup/","title":"起步文档","keywords":"","body":"起步文档 本章节会列出几种不同的部署并运行k8s集群的方案。当您部署k8s集群的时，请参考以下几个方面作出选择：维护成本、安全性、控制难易、可用资源以及专业知识。 您将k8s集群部署到本地机器、云厂商、数据中心或者选择一个已经部署好的集群。当然，自定义的方案还有云服务提供商和裸机原生环境等。 学习环境 如果您正在学习k8s，建议使用k8s社区支持的工具，或者使用k8s生态中的工具在本地部署一套集群。 生产环境 当应用到生产环境中的时候，请考虑自己的操作技能掌握，或者托管给第三方提供商。 "},"setup/release.html":{"url":"setup/release.html","title":"发版概要","keywords":"","body":"发版提示与版本概要 v1.20 Release Notes Kubernetes version and version skew support policy "},"tasks/tools.html":{"url":"tasks/tools.html","title":"学习环境","keywords":"","body":"安装工具 "},"setup/production-environment.html":{"url":"setup/production-environment.html","title":"生产环境","keywords":"","body":"生产环境介绍 Container runtimes Installing Kubernetes with deployment tools Turnkey Cloud Solutions Windows in Kubernetes "},"setup/best-practices/":{"url":"setup/best-practices/","title":"最佳实践","keywords":"","body":"最佳实践 Considerations for large clusters Running in multiple zones Validate node setup PKI certificates and requirements "},"setup/best-practices/cluster-large.html":{"url":"setup/best-practices/cluster-large.html","title":"考虑大型集群","keywords":"","body":"考虑大型集群 一个集群是由一系列运行k8s客户端的节点（物理机器或者虚拟机）组成，该集群受到控制平面的管理，k8s1.20版本支持高达5000个节点的集群。更具体地来说，k8s基于以下标准设计： 每个节点不超过100个容器组 集群不超过5000个节点 集群总共不超过150000个容器组 集群总共不抄错300000个容器 您可以通过增加或者移除节点实现集群的伸缩，能否达成该点取决于集群的部署方式。 云服务厂商的资源限制 为了避免遇到云服务提供上限额的问题，当常见多节点的集群时候，需要考虑以下几点： 为云资源请求增长的限额，例如：计算实例，CPU核数，存储卷，固定IP，数据包过滤规则，负载均衡器，子网段数量，日志流； 为增加新节点预留空间，因为一些云服务厂商会限制新建实例的速率。 控制平面组件 对于大型集群来说，你需要一个拥有强大算力和其他资源的控制平面。一般情况下，你需要为每一个故障域运行1个到2个控制平面。需要时，先对实例做垂直缩放，当达到极限后再做水平缩放。每个故障域你需要运行至少一个实例来提供容错机制。k8s节点不会自动将流量引向相同故障区域中的控制平面端点，但是您的云服务提供商可能有他自己的实现。例如，使用托管的负载均衡器，你可以配置负载均衡器发送源自故障域A中的kubelet和pod的流量，到也位于区域A中的控制平面主机。如果单节点的控制平面发生故障，导致区域A离线，则会导致区域A中节点的所有控制平面流量现在都在区域之间发送。在每个域中运行多个控制平面可以减少这种情况产生。 etcd存储 为了提高大型集群的性能，您可以通过将事件信息独立存储到etcd实例中。当创建集群的时候，你可以： 启用并配置额外的etcd实例 配置apiserver使用etcd存储事件信息 插件资源 k8s资源限制特性使得内存泄漏和来自pod或者容器的影响最小的影响到其他组件。这些资源限制可以并且应该作用于插件上，正如它们作用在工作负载上一样。例如，你可以为一个日志组件设置CPU和内存限制： ... containers: - name: fluentd-cloud-logging image: fluent/fluentd-kubernetes-daemonset:v1 resources: limits: cpu: 100m memory: 200Mi 插件的默认限制一般基于实践中中小型集群运行插件的经验数据。当运行大型集群的时候，插件经常会消耗比默认限额多很多的资源。如果一个大型集群部署的时候没有调整这些参数，这些插件会不停的由于达到内存限额而被杀死，同理，这些插件也许会由于CPU时间片的限额而运行在低性能下。 为了避免遇到插件的资源限制问题，当创建一个多节点的集群的时候，您需要考虑以下几点： 一些插件垂直扩展-那些一个故障域只需要单个副本的插件；对于这些插件，请在扩展集群的时候增加资源和限制。 大多数插件水平扩展-那些可以通过运行多个pod提升容量的插件；但是针对一个超大的几集群，你可能也需要同时轻微的增加CPU和内存限制。 VerticalPodAutoscaler 运行在recommender模式下可以为请求与限制提供数额建议。 一些插件需要一个节点运行一个-那些通过DaemonSet部署的插件；例如，一个节点级别的日志聚合器，类似于需要水平扩展的插件，这些插件也需要轻微的增加CPU和内存限制。 其他 VerticalPodAutoscaler是一个自定义资源，你可以部署到集群汇总帮助管理pod的资源请求与限制。 cluster autoscaler通过与一系列的云服务提供者交互，帮助你的集群运行足够数量的节点。 "},"setup/best-practices/multiple-zones.html":{"url":"setup/best-practices/multiple-zones.html","title":"多域环境运行","keywords":"","body":"多域环境运行 本节讲解在k8s中运行多个环境。 背景 经过设计，k8s集群可以应对多个故障域问题的情况，一般来说这些域对应到逻辑组中叫做region。主流的云平台定义了地域为一系列故障域的集合，并提供一系列的特色服务：同一地域中，每个故障域提供相同的接口和服务。 控制平面行为 所有的控制平面组件都支持作为可互换资源池运行，每个组件都可被复制。部署集群控制平面时，请将控制平面组件的副本放置在多个故障区域中。如果可用性是一个重要问题，请选择至少三个故障区域，并在至少三个故障区域中复制每个单独的控制平面组件（API服务器，调度程序等，群集控制器管理器）。如果您使用了云厂商提供的控制器，您也应当保证故障域有多个副本。 注意 Kubernetes不为API服务器端点提供跨区域弹性。 您可以使用各种技术来提高群集API服务器的可用性，包括DNS轮询，SRV记录或具有运行状况检查的第三方负载平衡解决方案。 节点行为 节点启动时，每个节点上的kubelet会自动的将标签添加到代表kubernetes api中该特定的kubelet的Node对象。这些标签信息可以包含域信息。如果你的集群跨多个域分布，建议将节点标签搭配pod扩展约束一起使用，以控制pod如何在整个故障域之间分布。这样操作可以使得调度器将pod调度到一个理想的可用性状态，减少相关的出错影响整个工作负载带来的风险。例如，条件允许时，你可以设置限制，将3个副本的StatefulSet应用部署到不同的域中。你可以生命性的定义它，而无需显示定义每个工作负载正在使用哪些域。 节点的跨域分布 k8s本身并不会为您创建节点，你需要自己来执行此操作，或者使用注入Cluster API之类的工具来管理你的节点。使用Cluster API之类的工具可以定义一系列机器集合，这些机器集合作为工作节点分布运行在多个故障域中，也会根据规则自愈集群。 管理域为pod的划分 您可以为你创建的pod设置节点标签限制器，同理，在工作负载类的资源的pod模板中也可以这样操作，例如Deployment,StatefulSet,Job。 存储的跨域 当持久化卷创建时，管理控制器会自动的为关联到域中的持久化卷增加PersistentVolumeLabel标签。调度器随后会做确认，通过NoVolumeZoneConflict来确认pod可以声明一个同域中的持久化卷。你可以为持久化卷声明指定一个存储控制器，该类指定该类中的存储可以使用的故障域。 网络 k8s本身并不支持无感网络，您可以使用网络插件来配置集群网络，并且该网络解决方案可能具有特定于区域的元素。例如，如果您的云服务提供商支持type=LoadBalancer的代理，则负载均衡器可能仅将流量发送到与处理同一连接的负载均衡器的同一个区域中的pod。关于这点，请查阅云厂商的文档。对于自定义或者本地部署的情况，也有类似的注意事项。Service和Ingress的行为，包括如何处理不同的故障与，很大程度上取决于您的集群的部署方式。 错误恢复 当部署集群的时候，你也许需要考虑下是否以及如何让你的集群可以在多个故障域同时离线的情况下恢复，例如，你依赖于每个故障域至少有一个运行pod的节点吗？要知道，任何集群成对的工作不会依赖于集群中至少一个健康节点。例如，如果所有节点都处于不健康状态，你也许需要运行一个修复任务，这个任务会需要一个特殊的容忍度，才能够实现修复至少一个节点的服务。针对这个问题，k8s并没有给出解答，但是这仍然是需要考量的。 "},"setup/best-practices/node-conformance.html":{"url":"setup/best-practices/node-conformance.html","title":"校验节点部署","keywords":"","body":"校验节点部署 节点一致性测试 节点一致性测试是一个容器化的测试框架，可为节点提供系统验证和功能测试。 该测试将验证该节点是否满足Kubernetes的最低要求； 通过测试的节点有资格加入Kubernetes集群。 节点先觉条件 要想运行节点一致性测试，节点必须满足于标准化k8s节点相同的先决条件。至少，该节点需要运行以下守护程序： Container Runtime Kubelet 运行节点一致性测试 运行节点一致性测试，请按照以下步骤操作： 确认kubelet的--kubeconfig值；比如，指定--kubeconfig=/var/lib/kubelet/config.yaml。原因在于测试框架使用本地控制平面来测试kubelet，使用http://localhost:8080作为APIServer的地址。其他可能用到的kubelet命令行参数如下： --pod-cidr: 如果使用了kubenet，需要为kubelet指定一个随机的CIDR，例如：--pod-cidr=10.180.0.0/24。 --cloud-provider：如果使用了参数--cloud-provider=gce，运行前需要移除该参数。 运行以下命令进行测试： # $CONFIG_DIR is the pod manifest path of your Kubelet. # $LOG_DIR is the test output path. sudo docker run -it --rm --privileged --net=host \\ -v /:/rootfs -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\ k8s.gcr.io/node-test:0.2 其他架构下的一致性测试 k8s为其他架构提供了一致性测试docker镜像： arch image amd64 node-test-amd64 arm node-test-arm arm64 node-test-arm64 运行部分测试 要运行特定的测试，请用要运行的测试的正则表达式覆盖环境变量FOCUS: sudo docker run -it --rm --privileged --net=host \\ -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\ -e FOCUS=MirrorPod \\ # Only run MirrorPod test k8s.gcr.io/node-test:0.2 要跳过特定的测试，请用要跳过的测试的正则表达式覆盖环境变量SKIP: sudo docker run -it --rm --privileged --net=host \\ -v /:/rootfs:ro -v $CONFIG_DIR:$CONFIG_DIR -v $LOG_DIR:/var/result \\ -e SKIP=MirrorPod \\ # Run all conformance tests but skip MirrorPod test k8s.gcr.io/node-test:0.2 节点一致性测试是节点e2e测试的容器化版本。默认情况下，它将运行所有一致性测试。从理论上讲，如果您配置了容器并正确安装了所需的卷，则可以运行任何节点e2e测试。 但强烈建议仅运行一致性测试，因为它需要更复杂的配置才能运行不一致性测试。 注意事项 该项测试会在节点上留下docker镜像，包含一致性测试镜像和功能测试的容器 测试会在节点上留下停止运行的容器，这些容器在功能测试的过程中创建。 "},"setup/best-practices/certificates.html":{"url":"setup/best-practices/certificates.html","title":"PKI证书问题","keywords":"","body":"PKI证书问题 k8s需要基于TLS的PKI证书授权。如果您使用了kubeadm部署的k8s，集群会自动生成需要的证书。当然，你也可以自己手动创建证书。例如，为了保证私有证书更加安全，你可以将其不存放在API Server中。本文讲解集群需要的证书。 集群是如何使用证书的 k8s在以下操作的时候需要用到PKI认证： kubelet访问API Server需要客户端证书 APIserver 端点需要服务端证书 集群管理员认证API server需要客户端证书 API server与kubelet通信需要客户端证书 API server与etcd通信需要客户端证书 controller manager与API server通信时候需要客户端证书和kubeconfig scheduler与API server通信时候需要客户端证书和kubeconfig 前置代理需要客户端和服务端证书 注意 前置代理需要证书的场景仅存在于运行kube-proxy支撑APIserver扩展的时候。 etcd在授权客户端和节点的时候也实现了一般的TLS认证 证书的存放位置 使用kubeadm安装k8s的时候，证书存放在/etc/kubernetes/pki，本文中提到的所有路径都是相对于该目录。 手动配置证书 如果你不想kubeadm自动生成证书的话，你可以使用以下几种方式创建： 单独根CA证书 你可以使用管理员创建一个根证书。然后，该根CA可以创建多个中间CA，并将所有进一步的创建委托给Kubernetes本身。 需要的CA： 路径 默认cn 描述 ca.crt,key kubernetes-ca Kubernetes general CA etcd/ca.crt,key etcd-ca For all etcd-related functions front-proxy-ca.crt,key kubernetes-front-proxy-ca For the front-end proxy 在上述CA之上，还需要获取用于服务帐户管理的公共/私有密钥对sa.key和sa.pub。 所有证书 如果你不想复制这些证书到你的集群中，你可以自己生成这些证书。 需要的证书如下： 默认CN 父证书 O 类型 hosts kube-etcd etcd-ca server, client localhost, 127.0.0.1 kube-etcd-peer etcd-ca server, client hostname, Host_IP, localhost, 127.0.0.1 kube-etcd-healthcheck-client etcd-ca client kube-apiserver-etcd-client etcd-ca system:masters client kube-apiserver kubernetes-ca server hostname, Host_IP, advertise_IP, [1] kube-apiserver-kubelet-client kubernetes-ca system:masters client front-proxy-client kubernetes-front-proxy-ca client [1]： 表示访问集群的其他任何IP或者DNS名称，其中kind映射到一种或多种x509密钥用法类型： kind 密钥用法 server digital signature, key encipherment, server auth client digital signature, key encipherment, client auth 注意：上面列出的主机/SAN是获得工作群集的推荐主机； 如果特定设置需要，则可以在所有服务器证书上添加其他SAN。 注意：对于kubeadm用户而言只需要如下： 在不使用私钥的情况下复制到群集CA证书的方案在kubeadm文档中称为外部CA。 如果将上面的列表与kubeadm生成的PKI进行比较，请注意，如果使用外部etcd，则不会生成kube-etcd，kube-etcd-peer和kube-etcd-healthcheck-client证书。 证书路径 证书应当存放在推荐的路径下，路径需要通过特定的参数指定而不是路径。 Default CN recommended key path recommended cert path command key argument cert argument etcd-ca etcd/ca.key etcd/ca.crt kube-apiserver --etcd-cafile kube-apiserver-etcd-client apiserver-etcd-client.key apiserver-etcd-client.crt kube-apiserver --etcd-keyfile --etcd-certfile kubernetes-ca ca.key ca.crt kube-apiserver --client-ca-file kubernetes-ca ca.key ca.crt kube-controller-manager --cluster-signing-key-file --client-ca-file, --root-ca-file, --cluster-signing-cert-file kube-apiserver apiserver.key apiserver.crt kube-apiserver --tls-private-key-file --tls-cert-file kube-apiserver-kubelet-client apiserver-kubelet-client.key apiserver-kubelet-client.crt kube-apiserver --kubelet-client-key --kubelet-client-certificate front-proxy-ca front-proxy-ca.key front-proxy-ca.crt kube-apiserver --requestheader-client-ca-file front-proxy-ca front-proxy-ca.key front-proxy-ca.crt kube-controller-manager --requestheader-client-ca-file front-proxy-client front-proxy-client.key front-proxy-client.crt kube-apiserver --proxy-client-key-file --proxy-client-cert-file etcd-ca etcd/ca.key etcd/ca.crt etcd --trusted-ca-file, --peer-trusted-ca-file kube-etcd etcd/server.key etcd/server.crt etcd --key-file --cert-file kube-etcd-peer etcd/peer.key etcd/peer.crt etcd --peer-key-file --peer-cert-file etcd-ca etcd/ca.crt etcdctl --cacert kube-etcd-healthcheck-client etcd/healthcheck-client.key etcd/healthcheck-client.crt etcdctl --key --cert 相同的注意事项适用于服务帐户密钥对： private key path public key path command argument sa.key kube-controller-manager --service-account-private-key-file sa.pub kube-apiserver --service-account-key-file 用户账户的证书配置 你必须要配置这些管理员账户和服务账户： filename credential name Default CN O (in Subject) admin.conf default-admin kubernetes-admin system:masters kubelet.conf default-auth system:node: (see note) system:nodes controller-manager.conf default-controller-manager system:kube-controller-manager scheduler.conf default-scheduler system:kube-scheduler 注意：kubelet.conf的的值必须与kubelet向apiserver注册时提供的节点名称的值完全匹配。 有关更多详细信息，请阅读节点授权。 对于每个配置，请使用给定的CN和O生成一个x509证书/密钥对。 对每个配置运行kubectl，如下所示： KUBECONFIG= kubectl config set-cluster default-cluster --server=https://:6443 --certificate-authority --embed-certs KUBECONFIG= kubectl config set-credentials --client-key .pem --client-certificate .pem --embed-certs KUBECONFIG= kubectl config set-context default-system --cluster default-cluster --user KUBECONFIG= kubectl config use-context default-system 这些文件的用途如下： filename command comment admin.conf kubectl Configures administrator user for the cluster kubelet.conf kubelet One required for each node in the cluster. controller-manager.conf kube-controller-manager Must be added to manifest in manifests/kube-controller-manager.yaml scheduler.conf kube-scheduler Must be added to manifest in manifests/kube-scheduler.yaml "},"concepts/":{"url":"concepts/","title":"内容详解","keywords":"","body":"内容详解 本章节帮助你了解Kubernetes系统的各个部分以及Kubernetes用于表示集群的抽象，并有助于您更深入地了解Kubernetes的工作原理。 集群概览 了解Kubernetes及其构建组件的高级概述。 集群架构 Kubernetes背后的架构概念。 容器详解 用于打包应用程序及其运行时依赖项的技术。 工作负载 了解Pod，这是Kubernetes中最小的可部署计算对象，以及有助于您运行它们的更高级抽象。 代理、负载均衡与网络 Kubernetes中网络背后的概念和资源。 后端存储 几种为你集群中的pod提供长期和临时的存储的方式 配置定义 kubernetes为pod提供配置文件资源 安全策略 保证云原声负载安全的概念 资源策略 您可以配置的策略适用于资源组。 调度驱逐 在Kubernetes中，调度是指确保Pod与Node匹配，以便kubelet可以运行它们。 驱逐是在资源匮乏的节点上主动使一个或多个Pod发生故障的过程。 集群管理 与创建或管理Kubernetes集群有关的低级详细信息。 功能扩展 多种方式改变集群行为 "},"concepts/overview/":{"url":"concepts/overview/","title":"集群概览","keywords":"","body":"集群概览 什么是k8s k8s是一个便携的，可扩展的，以及开源的平台，用来管理容器化的工作负载和服务，该基础设施兼具声明式配置和自动化特性。k8s是一个大型的，快速成长的生态系统。使用k8s服务，支持以及工具，具备广泛的可行性。 k8s组件 k8s集群由代表控制平面的组件以及机器集合组成的node节点组件组成。 k8s api k8s api允许你查询和管理集群对象的状态。k8s核心的控制平面包含由api server以及http api暴露出来的接口。用户之间，集群中的不同部分，以及外部组件之间的所有通信都是通过api server完成。 理解k8s对象 k8s对象是k8s系统中的持久化内容。k8s使用这些内容代表集群。学习k8s对象模型，以及如何使用这些对象。 "},"concepts/overview/what-is-kubernetes.html":{"url":"concepts/overview/what-is-kubernetes.html","title":"什么是k8s","keywords":"","body":"什么是k8s 当前页面是k8s的简介。 k8s是一个便携的，可扩展的，以及开源的平台，用来管理容器化的工作负载和服务，该基础设施兼具声明式配置和自动化特性。k8s是一个大型的，快速成长的生态系统。使用k8s服务，支持以及工具，具备广泛的可行性。 Kubernetes这个名字起源于希腊语，意思是舵手或飞行员。Google于2014年开源了kubernetes项目。Kubernetes将超过15年的Google在大规模生产工作负载方面的经验与社区中最好的想法和实践相结合。 回溯以往 想要理解kubernetes为什么这么有用处，让我们把时间往前推。 传统的部署：早期，企业一般在物理机上运行应用。在物理机上没有办法解决资源的依赖，由此导致了资源的分配问题。例如，如果多个应用运行在同一个物理机上，某些实例可能会出现单个应用占用几乎全部资源的情况，带来的结果就是，其他的应用表现不佳。这种现象的解决方案之一是应用运行在不同的物理机上。但是，这样的话当资源利用不足的时候无法缩小资源，同时企业管理很多的物理机器是非常昂贵的。 *虚拟化部署：虚拟化部署作为一个解决方案被提了出来。该方案允许在单个物理服务器上运行多个虚拟的机器。虚拟化方案将应用隔离在了虚拟机层面，并且提供了一定程度的安全性，因为一个应用的信息无法随意的被另外一个应用访问。 虚拟化方案提供了比物理机器更好的资源利用率，以及更好的可扩展性，因为一个应用可以很容易的新增和更新，减少了硬件的开销以及其他开销。通过虚拟化方案，你可以设置一组物理资源作为虚拟机集群。 每一个虚拟机都是一个运行所有组件的机器，包含虚拟化的硬件以及之上的操作系统组件， 容器部署：容器话和虚拟机很类似，但是它们可以更加轻松的在应用间隔离操作系统。因此，容器被认为是更加轻量级的。类似于虚拟机，一个容器有其自身的文件系统，共享的cpu，共享的内存，进程空间，以及其他。因为它们与基础架构的分离，所以可以跨云和OS进行分发。 容器化变得流行，是因为它们提供了额外的好处，例如： 敏捷的应用创建与部署：相比于虚拟机镜像的使用，容器化提升了效率 持续开发，持续继承，以及持续部署：通过快速有效的回滚提供可靠且频繁的容器映像构建和部署 开发和运维的关注点分离：在构建/发版的时候创建应用镜像，而不是开发的时候，由此解偶了应用月基础架构 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他信号。 跨开发，测试和生产的环境一致性：在笔记本电脑上与在云中一样在笔记本电脑上运行。 云和操作系统分发的可移植性：可在Ubuntu，RHEL，CoreOS，本地，主要公共云以及其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行操作系统到使用逻辑资源在操作系统上运行应用程序。 松散耦合，分布式，弹性，解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理–而不是在一台大型单机上运行的单片堆栈。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。 为什么你需要kubernetes以及它可以做什么 容器化是一个很好的方式去绑定并运行应用。在一个生产环境中，你需要管理运行应用的容器并且确保没有宕机时间。例如：如果一个容器宕机，另外一个容器需要启动。这种行为如果有系统来掌控是不是变得简单的多了？ 这就是k8s自愈的原理!k8s为你提供了一个弹性的分布式系统作为一个基础组件。它关注为你的应用扩容和故障转移，提供了部署模式以及更多。例如：k8s可以很容易的为你的系统提供金丝雀发布。 kubernetes为你提供如下： 服务发现与负载均衡：k8s通过DNS名称或者它们自身的IP地址暴露容器。如果同一容器流量过高，kubernete能够均衡负载并且分发网络流量，保证部署服务的稳定性。 存储编排：Kubernetes允许您自动挂载您选择的存储系统，例如本地存储，公共云提供商等。 自动部署和回滚：您可以使用Kubernetes描述已部署容器的所需状态，它可以以受控的速率将实际状态更改为所需状态。 例如，您可以自动化Kubernetes来为您的部署创建新容器，删除现有容器并将其所有资源用于新容器。 自动资源回收：您为Kubernetes提供了一个节点集群，可用于运行容器化任务。 您告诉Kubernetes每个容器需要多少CPU和内存（RAM）。 Kubernetes可以将容器安装到您的节点上，以充分利用您的资源。 自愈：Kubernetes重新启动失败的容器，替换容器，杀死不响应用户定义的运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 密钥与配置管理：Kubernetes允许您存储和管理敏感信息，例如密码，OAuth令牌和SSH密钥。 您可以部署和更新机密信息和应用程序配置，而无需重建容器映像，也无需在堆栈配置中公开机密信息。 k8s不能做到什么 Kubernetes不是传统的，包罗万象的PaaS（平台即服务）系统。 由于Kubernetes在容器级别而非硬件级别运行，因此它提供了PaaS产品共有的一些普遍适用的功能，例如部署，扩展，负载平衡，并允许用户集成其日志记录，监视和警报解决方案。 但是，Kubernetes并不是单片的，并且这些默认解决方案是可选的和可插入的。 Kubernetes提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。 k8s: 不限制支持的应用程序类型。 Kubernetes旨在支持极为多样化的工作负载，包括无状态，有状态和数据处理工作负载。 如果应用程序可以在容器中运行，那么它应该可以在Kubernetes上很好地运行。 不部署源代码，也不构建您的应用程序。 持续集成，交付和部署（CI / CD）工作流取决于组织的文化和偏好以及技术要求。 不提供应用程序级别的服务，例如中间件（例如，消息总线），数据处理框架（例如，Spark），数据库（例如，MySQL），缓存或群集存储系统（例如，Ceph） 作为内置服务。 这样的组件可以在Kubernetes上运行，并且/或者可以由Kubernetes上运行的应用程序通过诸如开放服务代理之类的可移植机制来访问。 不指示日志记录，监视或警报解决方案。 它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。 不提供也不要求配置语言/系统（例如，Jsonnet）。 它提供了一个声明性API，可以通过任意形式的声明性规范作为目标。 不提供也不采用任何全面的机器配置，维护，管理或自我修复系统。 此外，Kubernetes不仅仅是一个编排系统。 实际上，它消除了编排的需要。 编排的技术定义是执行定义的工作流程：首先执行A，然后执行B，然后执行C。相反，Kubernetes包含一组独立的，可组合的控制过程，这些过程连续地将当前状态驱动到提供的所需状态。 从A到C的方式无关紧要。集中控制也不是必需的。 这使得系统更易于使用，功能更强大，更健壮，更具弹性和可扩展性。 "},"concepts/overview/components.html":{"url":"concepts/overview/components.html","title":"k8s组件","keywords":"","body":"k8s组件 当你部署kubernetes的时候，你得到的是一个集群。 一个kubernetes集群由一系列叫做工作机器的节点组成，在这些节点上运行容器化的应用。每个集群拥有至少一个工作节点。 工作节点托管pod，这些pod是应用程序工作负载的组成部分。控制平面负责管理工作节点，以及集群中的pod。生产环境下，控制平面经常运行在多个机器上，同时一个集群通常运行多个节点，这提供了容错性与高可用。 本文档概述了拥有完整且有效的Kubernetes集群所需的各种组件。 以下是Kubernetes集群的示意图，其中所有组件都捆绑在一起。 控制平面组件 控制平面的组件对集群作出全局性的决策，例如调度，同时可以监测并相应集群事件；例如，当deployment的副本数量不满足的时候，启动一个新的pod。 控制平面的组件可以运行在集群中的任一台机器上。但是，为简单起见，设置脚本通常在同一台计算机上启动所有控制平面组件，并且不在该计算机上运行用户容器。有关多主虚拟机设置示例，请参阅构建高可用性群集。 apiserver api server是Kubernetes控制平面的组件，该组件公开Kubernetes API。 api server是Kubernetes控制平面的前端。 Kubernetes API server的主要实现是kube-apiserver。 kube-apiserver旨在水平扩展-即，它通过部署更多实例进行扩展。 您可以运行kube-apiserver的多个实例，并平衡这些实例之间的流量。 etcd 一致且高度可用的键值存储用作所有集群数据的Kubernetes的后备存储。 如果您的Kubernetes集群使用etcd作为其后备存储，请确保您有针对这些数据的备份计划。 您可以在官方文档中找到有关etcd的详细信息。 kube-scheduler 控制平面组件，该组件监视没有分配节点的新创建的Pod，并选择一个节点以在其上运行。 计划决策要考虑的因素包括：个体和集体资源需求，硬件/软件/策略约束，亲和力和反亲和力规范，数据局部性，工作负载之间的干扰以及期限。 kube-controller-manager 运行控制器进程的控制平面组件。 从逻辑上讲，每个控制器是一个单独的进程，但是为了降低复杂性，它们都被编译为单个二进制文件并在单个进程中运行。 这些控制器的类型包含： 节点控制器：负责在节点出现故障时进行通知和响应。 作业控制器：监视代表一次性任务的作业对象，然后创建Pod以运行这些任务以完成任务。 端点控制器：填充“端点”对象（即，加入“服务和窗格”）。 服务帐户和令牌控制器：为新的名称空间创建默认帐户和API访问令牌。 cloud-controller-manager 嵌入了特定于云的控制逻辑的Kubernetes控制平面组件。 云控制器管理器使您可以将集群链接到云提供商的API，并将与该云平台交互的组件与仅与集群交互的组件分开。 cloud-controller-manager仅运行特定于您的云提供商的控制器。 如果您是在自己的场所或PC内的学习环境中运行Kubernetes，则该群集没有云控制器管理器。 与kube-controller-manager一样，cloud-controller-manager将多个逻辑上独立的控制循环组合为一个二进制文件，您可以将其作为单个进程运行。 您可以水平缩放（运行多个副本）以提高性能或帮助容忍故障。 以下控制器可以具有云提供程序依赖性： 节点控制器：用于检查云提供程序以确定节点停止响应后是否已在云中删除该节点 路由控制器：用于在基础云基础架构中设置路由 服务控制器：用于创建，更新和删除云提供商负载平衡器 工作节点组件 节点组件在每个节点上运行，维护运行中的Pod，并提供Kubernetes运行时环境。 kubelet 在集群中每个节点上运行的代理。 确保容器在Pod中运行。 kubelet包含通过各种机制提供的一组PodSpec，并确保这些PodSpec中描述的容器正在运行且状况良好。 Kubelet不管理不是Kubernetes创建的容器。 kube-proxy kube-proxy是一个网络代理，它在集群中的每个节点上运行，实现了Kubernetes Service概念的一部分。 kube-proxy维护节点上的网络规则。 这些网络规则允许从群集内部或外部的网络会话与Pod进行网络通信。 如果有kube-proxy可用，它将使用操作系统数据包过滤层。 否则，kube-proxy会转发流量本身。 Container runtime Container runtime是负责运行容器的软件。 Kubernetes支持几种容器运行时：Docker，容器化，CRI-O以及Kubernetes CRI（容器运行时接口）的任何实现。 插件 插件使用Kubernetes资源（DaemonSet，Deployment等）来实现集群功能。 由于这些功能提供集群级功能，因此插件的命名空间资源属于kube-system命名空间。 所选的插件说明如下： 有关可用插件的扩展列表，请参阅插件。 DNS 尽管并非严格要求其他附加组件，但由于许多示例都依赖于此，因此所有Kubernetes群集都应具有群集DNS。 除了您环境中的其他DNS服务器之外，群集DNS还是一个DNS服务器，它为Kubernetes服务提供DNS记录。 由Kubernetes启动的容器会在其DNS搜索中自动包括此DNS服务器。 仪表盘 仪表板是Kubernetes集群的基于Web的通用UI。 它允许用户管理集群中运行的应用程序以及集群本身并进行故障排除。 容器资源监控 容器资源监视在中央数据库中记录有关容器的一般时间序列指标，并提供用于浏览该数据的UI。 集群级别的日志 集群级别的日志记录机制负责通过搜索/浏览界面将容器日志保存到中央日志存储中。 "},"concepts/overview/kubernetes-api.html":{"url":"concepts/overview/kubernetes-api.html","title":"k8s api","keywords":"","body":"k8s api kubernetes控制平面的核心是api server。api server暴露出HTTP API，这保证了终端用户、集群间的不同部分、以及外部组件，之间互相通信。 Kubernetes API使您可以查询和操纵Kubernetes中API对象的状态（例如：Pods，命名空间，ConfigMap和Events）。 多数操作可以通过kubectl命令行界面或其他命令行工具（例如kubeadm）执行，这些工具依次使用API。 但是，您也可以使用REST调用直接访问API。 如果要使用Kubernetes API编写应用程序，请考虑使用其中一种客户端库。 OpenAPI 特性 使用OpenAPI记录了完整的API详细信息。 Kubernetes API服务器通过/ openapi / v2端点提供OpenAPI规范。 您可以使用请求标头来请求响应格式，如下所示： Header Possible values Notes Accept-Encoding gzip not supplying this header is also acceptable Accept application/com.github.proto-openapi.spec.v2@v1.0+protobuf mainly for intra-cluster use application/json default * serves application/json Kubernetes实现了另一种基于Protobuf的序列化格式，该格式主要用于集群内通信。 有关此格式的更多信息，请参阅Kubernetes Protobuf序列化设计建议以及位于定义API对象的Go包中的每个架构的接口定义语言（IDL）文件。 持久化 Kubernetes通过将对象的序列化状态写入etcd来存储它们。 API 分组与版本 为了更轻松地消除字段或重组资源表示形式，Kubernetes支持多个API版本，每个版本位于不同的API路径，例如/ api / v1或/apis/rbac.authorization.k8s.io/v1alpha1。 版本控制是在API级别而不是资源或字段级别完成的，以确保API呈现系统资源和行为的清晰一致的视图，并能够控制对寿命终止和/或实验性API的访问。 为了使其易于开发和扩展其API，Kubernetes实现了可以启用或禁用的API组。 API资源通过其API组，资源类型，名称空间（用于命名空间的资源）和名称来区分。 API服务器透明地处理API版本之间的转换：所有不同版本实际上是相同持久数据的表示。 API服务器可以通过多个API版本提供相同的基础数据。 例如，假设对于同一资源有两个API版本，即v1和v1beta1。 如果最初使用其API的v1beta1版本创建了对象，则以后可以使用v1beta1或v1 API版本读取，更新或删除该对象。 API变动 任何成功的系统都需要随着新用例的出现或现有用例的变化而增长和变化。 因此，Kubernetes设计了Kubernetes API来不断变化和增长。 Kubernetes项目旨在不破坏与现有客户端的兼容性，并在一段时间内保持这种兼容性，以便其他项目有机会进行调整。 通常，可以频繁地添加新的API资源和新的资源字段。 消除资源或字段要求遵循API弃用政策。 一旦正式的Kubernetes API达到通用版本（GA）（通常在API版本v1），Kubernetes便会保持其兼容性。 此外，即使在可行的情况下，Kubernetes仍可与Beta API版本保持兼容性：如果您采用Beta API，即使功能稳定后，您仍可以继续使用该API与集群进行交互。 注意：尽管Kubernetes还旨在保持与Alpha API版本的兼容性，但是在某些情况下这是不可能的。 如果您使用任何Alpha API版本，请在升级集群时查看Kubernetes的发行说明，以防API确实发生了变化。 API扩展 Kubernetes API可以通过以下两种方式之一进行扩展： 使用自定义资源，您可以声明性地定义API服务器应如何提供所选资源API。 您还可以通过实现聚合层来扩展Kubernetes API。 "},"concepts/overview/working-with-objects/":{"url":"concepts/overview/working-with-objects/","title":"k8s 资源对象","keywords":"","body":"k8s 资源对象 理解kubernetes对象 kubernetes对象管理 对象名称和ID 名称空间 标签与选择器 注解 字段选择器 推荐的字段 "},"concepts/overview/working-with-objects/kubernetes-objects.html":{"url":"concepts/overview/working-with-objects/kubernetes-objects.html","title":"理解k8s对象","keywords":"","body":"理解k8s对象 本文介绍了如何在Kubernetes API中表示Kubernetes对象，以及如何以.yaml格式表示它们。 理解k8s对象 Kubernetes对象是Kubernetes系统中的持久实体。 Kubernetes使用这些实体来表示集群的状态。 具体来说，它们可以描述： 哪些容器化的应用程序正在运行（以及在哪些节点上） 这些应用程序可用的资源 有关这些应用程序的行为的策略，例如重新启动策略，升级和容错 Kubernetes对象是“意图记录”-创建对象后，Kubernetes系统将不断工作以确保该对象存在。 通过创建一个对象，您可以有效地告诉Kubernetes系统您希望集群的工作负荷是什么样子。 这是您的集群的期望状态。 要使用Kubernetes对象-无论是创建，修改还是删除它们-您都需要使用Kubernetes API。 例如，当您使用kubectl命令行界面时，CLI会为您进行必要的Kubernetes API调用。 您还可以使用客户端库之一在自己的程序中直接使用Kubernetes API。 对象特性和状态 几乎每个Kubernetes对象都包含两个嵌套的对象字段，这些字段控制对象的配置：对象规范和对象状态。 对于具有规格的对象，必须在创建对象时进行设置，并提供所需资源的特征描述：所需状态。 状态描述了对象的当前状态，该状态由Kubernetes系统及其组件提供和更新。 Kubernetes控制平面连续不断地主动管理每个对象的实际状态，以匹配您提供的所需状态。 例如：在Kubernetes中，Deployment是一个对象，可以表示您在集群上运行的应用程序。 创建展开时，可以将展开规范设置为指定要运行该应用程序的三个副本。 Kubernetes系统读取Deployment规范并启动所需应用程序的三个实例-更新状态以符合您的规范。 如果这些实例中的任何一个都应该失败（状态更改），则Kubernetes系统将通过进行更正来响应规范和状态之间的差异-在这种情况下，将启动替换实例。 有关对象规范，状态和元数据的更多信息，请参见Kubernetes API约定。 描述kubernetes对象 在Kubernetes中创建对象时，必须提供描述其所需状态的对象规范以及有关该对象的一些基本信息（例如名称）。 当您使用Kubernetes API创建对象（直接或通过kubectl）时，该API请求必须在请求正文中包含该信息作为JSON。 通常，您会在.yaml文件中将信息提供给kubectl。 发出API请求时，kubectl会将信息转换为JSON。 这是一个示例.yaml文件，显示了Kubernetes部署的必填字段和对象规范： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 使用.yaml文件创建展开的一种方法是使用上述方法，是在kubectl命令行界面中使用kubectl apply命令，并将.yaml文件作为参数传递。 这是一个例子： kubectl apply -f https://k8s.io/examples/application/deployment.yaml --record 输出类似如下： deployment.apps/nginx-deployment created 必备字段 在您要创建的Kubernetes对象的.yaml文件中，您需要为以下字段设置值： apiVersion：您正在使用哪个版本的Kubernetes API创建该对象 kind：你想要创建那种类型的对象 metadata：有助于唯一表示对象的数据，例如name,UID,namespace spec：你定义的对象期望状态 每个Kubernetes对象的对象规范的精确格式都不同，并且包含特定于该对象的嵌套字段。 Kubernetes API参考可以帮助您找到可以使用Kubernetes创建的所有对象的规范格式。 例如，可以在PodSpec v1核心中找到Pod的规范格式，而可以在DeploymentSpec v1应用中找到Deployment的规范格式。 "},"concepts/overview/working-with-objects/object-management.html":{"url":"concepts/overview/working-with-objects/object-management.html","title":"对象的管理","keywords":"","body":"对象的管理 kubectl命令行工具支持几种不同的方式来创建和管理Kubernetes对象。 本文档概述了不同的方法。 阅读Kubectl书，了解Kubectl管理对象的详细信息。 管理技巧 警告：应该仅使用一种技术来管理Kubernetes对象。 同一对象的混合和匹配技术会导致不确定的行为。 管理技巧 操作对象 推荐环境 支持的作者 学习曲线 命令式命令 实时对象 开发项目 多于1种 最低 命令式对象配置 单独文件 生产环境 1种 中等 声明式对象配置 文件夹 生产环境 多余1种 最高 命令式命令 使用命令式命令时，用户可以直接在集群中的活动对象上进行操作。 用户将对kubectl命令的操作作为参数或标志提供给用户。 这是在集群中开始或运行一次性任务的推荐方法。 因为此技术直接在活动对象上运行，所以它不提供先前配置的历史记录。 示例 通过创建Deployment对象来运行nginx容器的实例： kubectl create deployment nginx --image nginx 权衡取舍 与命令式对象配置相比的优势： 命令被表示为单个动作词。 命令仅需一步即可对集群进行更改。 与对象配置相比的缺点： 命令不与变更审查流程集成。 命令不提供与更改关联的审核跟踪。 除实时内容外，命令不提供记录源。 命令不提供用于创建新对象的模板。 命令式对象配置 在命令性对象配置中，kubectl命令指定操作（创建，替换等），可选标志和至少一个文件名。 指定的文件必须包含YAML或JSON格式的对象的完整定义。 警告：命令替换命令将现有规范替换为新提供的规范，并删除对配置文件中缺少的对象的所有更改。 此方法不应与规格独立于配置文件进行更新的资源类型一起使用。 例如，LoadBalancer类型的服务的externalIPs字段独立于集群的配置进行更新。 示例 创建定义在配置文件中的对象： kubectl create -f nginx.yaml 删除定义在2个配置文件中的对象： kubectl delete -f nginx.yaml -f redis.yaml 通过覆盖配置的方式更新定义在配置文件中的对象： kubectl replace -f nginx.yaml 权衡比较 相比命令交互式的优点如下： 对象的配置可以存放在源码控制系统例如git中 对象配置可以与流程集成，例如在推送和审计跟踪之前检查更改。 对象配置提供了用于创建新对象的模板。 相比于命令交互式的缺陷如下： 对象配置需要对对象架构有基本的了解。 对象配置需要额外的写入yaml文件的步骤。 相比于声明式配置的优点： 命令式对象配置行为更简单易懂。 从Kubernetes 1.5版开始，命令式对象配置更加成熟。 相比于声明式配置的缺点： 命令性对象配置最适合文件而不是目录。 对活动对象的更新必须反映在配置文件中，否则在下一次替换期间将丢失。 声明式配置 使用声明性对象配置时，用户对本地存储的对象配置文件进行操作，但是，用户未定义要对该文件执行的操作。 创建，更新和删除操作由kubectl自动检测到每个对象。 这样可以处理目录，其中不同的对象可能需要不同的操作。 注意：声明式对象配置保留其他编写者所做的更改，即使这些更改未合并回到对象配置文件中也是如此。 这可以通过使用补丁API操作仅写入观察到的差异，而不是使用replace API操作来替换整个对象配置来实现。 示例 处理configs目录中的所有对象配置文件，并创建或修补活动对象。 您可以先进行比较以查看将要进行的更改，然后应用： kubectl diff -f configs/ kubectl apply -f configs/ 递归处理目录： kubectl diff -R -f configs/ kubectl apply -R -f configs/ 权衡比较 相对于命令式对象配置的优点： 即使没有将它们直接合并回配置文件，也将保留直接对活动对象所做的更改。 声明性对象配置更好地支持对目录进行操作并自动检测每个对象的操作类型（创建，修补，删除）。 相对于命令式对象配置的缺点： 声明性对象配置在意外发生时更难调试和理解结果。 使用差异的部分更新会创建复杂的合并和补丁操作。 "},"concepts/overview/working-with-objects/names.html":{"url":"concepts/overview/working-with-objects/names.html","title":"对象的名称和ID","keywords":"","body":"对象的名称和ID 集群中的每个对象都有其该类型的唯一名称。每一个kubernetes对象都有一个在整个集群唯一的UUID。 名称 一个客户端提供的字符串类型，指向该对象的资源链接，例如/api/v1/pods/some-name。 同一时间，只有给定的类型的对象可以拥有一个给定的名称。但是，如果你删除了这个对象，你可以重新创建同名的对象。 以下是3种命名资源的常用方式。 DNS子域名称 大多数资源类型必须包含可以用作DNS子域名的名字，符合RFC 1123标准。这意味着这个名字必须包含： 不多余253个字符 只包含消协字母数字，以及-,. 字母数字开头 字母数字结束 DNS标签名称 某些资源类型要求其名称遵循RFC 1123中定义的DNS标签标准。这意味着名称必须： 最多63个字符 只包含消协字母数字，以及-,. 字母数字开头 字母数字结束 路径部分名称 某些资源类型要求其名称能够被安全地编码为路径段。 换句话说，名称不能为“。” 或“ ..”，并且名称中不能包含“ /”或“％”。 以下是一个名为nginx-demo的Pod的示例清单： apiVersion: v1 kind: Pod metadata: name: nginx-demo spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 注意： 某些资源类型对其名称有其他限制。 UIDs Kubernetes系统生成的字符串，用于唯一标识对象。 在Kubernetes集群的整个生命周期中创建的每个对象都有一个独特的UID。 旨在区分相似实体的历史发生。 Kubernetes UID是普遍唯一的标识符（也称为UUID）。 UUID被标准化为ISO / IEC 9834-8和ITU-T X.667。 "},"concepts/overview/working-with-objects/namespaces.html":{"url":"concepts/overview/working-with-objects/namespaces.html","title":"名称空间","keywords":"","body":"名称空间 Kubernetes支持由同一物理群集支持的多个虚拟群集。 这些虚拟集群称为名称空间。 何时使用多个名称空间 命名空间旨在用于具有多个用户的环境，这些用户分布在多个团队或项目中。 对于拥有几到几十个用户的集群，您根本不需要创建或考虑名称空间。 当需要名称空间提供的功能时，请开始使用它们。 命名空间为名称提供了作用域。 资源名称在名称空间中必须唯一，但在名称空间之间则必须唯一。 命名空间不能彼此嵌套，并且每个Kubernetes资源只能位于一个命名空间中。 命名空间是一种在多个用户之间（通过资源配额）划分群集资源的方法。 不必仅使用多个名称空间来分隔稍有不同的资源，例如同一软件的不同版本：使用标签来区分同一名称空间中的资源。 开始使用名称空间 名称空间管理指南中描述了名称空间的创建和删除。 注意：避免使用前缀kube-创建名称空间，因为它是为Kubernetes系统名称空间保留的。 查看 通过以下指令获取集群中的所有namespaces： kubectl get namespace NAME STATUS AGE default Active 1d kube-node-lease Active 1d kube-public Active 1d kube-system Active 1d kubernetes有4个初始化名称空间： default 没有其他名称空间的对象的默认名称空间 kube-system Kubernetes系统创建的对象的名称空间 kube-public 该名称空间是自动创建的，并且对所有用户（包括未经身份验证的用户）可读。 此名称空间主要保留给集群使用，以防某些资源在整个集群中公开可见。 此名称空间的公共方面仅是约定，不是要求。 kube-node-release 与每个节点关联的租用对象的此名称空间，可在群集扩展时提高节点心跳的性能。 设置请求的名称空间 为现有的请求设置名称空间， 请使用--namespace标签。 例如： kubectl run nginx --image=nginx --namespace= kubectl get pods --namespace= 设置默认名称空间 您可以为该上下文中的所有后续kubectl命令永久保存名称空间： kubectl config set-context --current --namespace= # Validate it kubectl config view --minify | grep namespace: 名称空间与DNS 创建服务时，它会创建一个相应的DNS条目。 此项的格式为。 .svc.cluster.local，这意味着如果容器仅使用，它将解析为名称空间本地的服务 。 这对于在多个名称空间（例如开发，登台和生产）中使用相同的配置很有用。 如果要跨命名空间访问，则需要使用完全限定的域名（FQDN）。 并不是所有的对象都属于名称空间 大多数Kubernetes资源（例如Pod，服务，复制控制器和其他资源）都位于某些命名空间中。 但是，名称空间资源本身并不在名称空间中。 而且低级资源（例如节点和persistentVolumes）不在任何命名空间中。 要查看哪些Kubernetes资源在命名空间中以及不在命名空间中： # In a namespace kubectl api-resources --namespaced=true # Not in a namespace kubectl api-resources --namespaced=false "},"concepts/overview/working-with-objects/labels.html":{"url":"concepts/overview/working-with-objects/labels.html","title":"标签和标签选择器","keywords":"","body":"标签和标签选择器 标签是附加到对象（例如pods）的键/值对。 标签旨在用于指定有意义且与用户相关的对象的标识属性，但并不直接暗示核心系统的语义。 标签可用于组织和选择对象的子集。 标签可以在创建时附加到对象，然后可以随时添加和修改。 每个对象可以定义一组键/值标签。 每个键对于给定的对象必须是唯一的。 \"metadata\": { \"labels\": { \"key1\" : \"value1\", \"key2\" : \"value2\" } } 标签允许进行有效的查询和监视，并且非常适合在UI和CLI中使用。 非识别信息应使用注释记录。 动机 标签使用户能够以松散耦合的方式将自己的组织结构映射到系统对象上，而无需客户端存储这些映射。 服务部署和批处理管道通常是多维实体（例如，多个分区或部署，多个发布轨道，多个层，每个层多个微服务）。 管理通常需要横切操作，这破坏了严格层次表示形式的封装，尤其是由基础结构而不是用户确定的刚性层次结构。 示例标签： \"release\" : \"stable\", \"release\" : \"canary\" \"environment\" : \"dev\", \"environment\" : \"qa\", \"environment\" : \"production\" \"tier\" : \"frontend\", \"tier\" : \"backend\", \"tier\" : \"cache\" \"partition\" : \"customerA\", \"partition\" : \"customerB\" \"track\" : \"daily\", \"track\" : \"weekly\" 这些是常用标签的示例； 您可以自由制定自己的约定。 请记住，标签Key对于给定的对象必须是唯一的。 语法和字符集 标签是键/值对。 有效的标签键分为两部分：可选的前缀和名称，用斜杠（/）分隔。 名称段是必需的，并且必须为63个字符或更少，以字母数字字符（[a-z0-9A-Z]）开头和结尾，并带有破折号（-），下划线（_），点（。），以及之间的字母数字 。 前缀是可选的。 如果指定，则前缀必须是DNS子域：一系列由点（。）分隔的DNS标签，总计不超过253个字符，后跟斜杠（/）。 如果省略前缀，则假定标签Key对用户是私有的。 向最终用户对象添加标签的自动化系统组件（例如kube-scheduler，kube-controller-manager，kube-apiserver，kubectl或其他第三方自动化）必须指定前缀。 The kubernetes.io/ and k8s.io/ prefixes are reserved for Kubernetes core components. 合法的标签值： 必须小于等于63个字符，且不能为空 以字母数字开头，[a-z0-9A-Z] 可以包含横线，下划线以及英文点，中间夹杂字母数字 例如，以下是一个pod的配置文件，包含2个标签：environment: production 和 app: nginx apiVersion: v1 kind: Pod metadata: name: label-demo labels: environment: production app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 标签选择器 与名称和UID不同，标签不提供唯一性。 通常，我们希望许多对象带有相同的标签。 通过标签选择器，客户端/用户可以识别一组对象。 标签选择器是Kubernetes中的核心分组原语。 该API当前支持两种选择器：基于等式和基于集合。 标签选择器可以由逗号分隔的多个要求组成。 在有多个要求的情况下，必须满足所有要求，以便逗号分隔符充当逻辑AND（&&）运算符。 空的或未指定的选择器的语义取决于上下文，并且使用选择器的API类型应记录它们的有效性和含义。 注意：对于某些API类型（例如ReplicaSets），两个实例的标签选择器在名称空间内一定不能重叠，否则控制器会将其视为冲突的指令，而无法确定应该存在多少个副本。 警告：对于基于相等的条件和基于集合的条件，都没有逻辑OR（||）运算符。 确保您的过滤器语句具有相应的结构。 基于平等的要求 基于等式或不等式的要求允许按标签键和值进行过滤。 匹配对象必须满足所有指定的标签约束，尽管它们也可能具有其他标签。 允许使用三种运算符=，==，！=。 前两个代表平等（并且是同义词），而第二个代表不平等。 例如： environment = production tier != frontend 前者选择等于环境且值等于生产的所有资源。 后者选择键等于层且值与前端不同的所有资源，并选择不带有层键的标签的所有资源。 一个人可以使用逗号运算符过滤生产中不包括前端的资源：environment = production，tier！= frontend 基于相等性的标签要求的一种使用场景是Pod指定节点选择标准。 例如，下面的示例Pod选择标签为“ accelerator = nvidia-tesla-p100”的节点。 apiVersion: v1 kind: Pod metadata: name: cuda-test spec: containers: - name: cuda-test image: \"k8s.gcr.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-tesla-p100 基于集合的需求 基于集合的标签要求允许根据一组值过滤关键字。 支持三种运算符：in，notin和存在（仅键标识符）。 例如： environment in (production, qa) tier notin (frontend, backend) partition !partition 第一个示例选择键等于环境且值等于生产或qa的所有资源。 第二个示例选择键等于tier的所有资源以及前端和后端以外的其他值，以及没有标签的所有带有tier key的资源。 第三个示例选择所有资源，包括带有键分区的标签； 不检查任何值。 第四个示例选择不带键分区标签的所有资源。 不检查任何值。 同样，逗号分隔符充当AND运算符。 因此，可以使用partition，environment notin（qa）来实现使用分区键（无论其值）和环境是否不同于qa来过滤资源。 基于集合的标签选择器是一般的相等形式，因为环境=生产等同于（生产）中的环境。 类似地用于！=和notin。 基于集合的需求可以与基于相等的需求混合。 例如：（客户，客户），环境中的分区！= qa。 API 列表和手表过滤 LIST和WATCH操作可以指定标签选择器以过滤使用查询参数返回的对象集。 这两个要求都是允许的（在此处显示，因为它们将出现在URL查询字符串中）： 基于等式的要求：？labelSelector = environment％3Dproduction，tier％3Dfrontend 基于集合的要求：？labelSelector =环境+ in +％28production％2Cqa％29％2Ctier + in +％28frontend％29 两种标签选择器样式均可用于通过REST客户端列出或监视资源。 例如，使用kubectl定位apiserver并使用基于等式的apiserver可能会这样写： kubectl get pods -l environment=production,tier=frontend 或使用基于集合的要求： kubectl get pods -l 'environment in (production),tier in (frontend)' 如前所述，基于集合的需求更具表现力。 例如，他们可以在值上实现OR运算符： kubectl get pods -l 'environment in (production, qa)' or restricting negative matching via exists operator: 或通过存在运算符限制否定匹配： kubectl get pods -l 'environment,environment notin (frontend)' 在API对象中设置引用 一些Kubernetes对象（例如服务和复制控制器）也使用标签选择器来指定其他资源集（例如Pod）。 服务和副本控制器 服务目标的Pod集合是使用标签选择器定义的。 同样，还使用标签选择器定义了复制控制器应管理的Pod数量。 使用地图在json或yaml文件中定义了两个对象的标签选择器，并且仅支持基于相等性的需求选择器： \"selector\": { \"component\" : \"redis\", } 或者： selector: component: redis 此选择器（分别为json或yaml格式）等效于component = redis或（redis）中的component。 支持基于集合的需求的资源 较新的资源（例如Job，Deployment，ReplicaSet和DaemonSet）也支持基于集合的要求。 selector: matchLabels: component: redis matchExpressions: - {key: tier, operator: In, values: [cache]} - {key: environment, operator: NotIn, values: [dev]} matchLabels是{key，value}对的映射。 matchLabels映射中的单个{key，value}等同于matchExpressions的元素，其key字段为“ key”，运算符为“ In”，而values数组仅包含“ value”。 matchExpressions是Pod选择器要求的列表。 有效的运算符包括In，NotIn，Exists和DidNotExist。 对于In和NotIn，设置的值必须为非空。 matchLabel和matchExpressions中的所有要求都进行了AND运算-必须全部满足才能匹配。 选择节点集 用于选择标签的一个用例是约束Pod可以调度到的节点集。 有关更多信息，请参见有关节点选择的文档。 "},"concepts/overview/working-with-objects/annotations.html":{"url":"concepts/overview/working-with-objects/annotations.html","title":"注解","keywords":"","body":"注解 您可以使用Kubernetes批注将任意非标识元数据附加到对象。 工具和库之类的客户端可以检索此元数据。 附加元数据到对象上 您可以使用标签或注释将元数据附加到Kubernetes对象。 标签可用于选择对象并查找满足特定条件的对象的集合。 相反，注释不用于标识和选择对象。 批注中的元数据可以是大小的，结构化的或非结构化的，并且可以包含标签不允许的字符。 注解类似于标签，是键值对的形式： \"metadata\": { \"annotations\": { \"key1\" : \"value1\", \"key2\" : \"value2\" } } 以下是一些注解中可以记录的信息示例： 由声明性配置层管理的字段。 将这些字段附加为注释可将其与客户端或服务器设置的默认值，自动生成的字段以及通过自动调整大小或自动缩放系统设置的字段区分开。 生成，发布或映像信息，例如时间戳，发行ID，git分支，PR号，图像哈希和注册表地址。 指向日志，监视，分析或审核存储库的指针。 可用于调试目的的客户端库或工具信息：例如，名称，版本和内部版本信息。 用户或工具/系统出处信息，例如来自其他生态系统组件的相关对象的URL。 轻量级推出工具元数据：例如，配置或检查点。 负责人的电话或寻呼机号码，或指定可在何处找到该信息的目录条目，例如团队网站。 从最终用户到实现的指令，以修改行为或使用非标准功能。 除了使用批注，您还可以将这种类型的信息存储在外部数据库或目录中，但这将使制作共享的客户端库和用于部署，管理，自省等的工具变得更加困难。 语法和字符集 注释是键/值对。 有效的注释键分为两部分：可选的前缀和名称，用斜杠（/）分隔。 名称段是必需的，并且必须为63个字符或更少，以字母数字字符（[a-z0-9A-Z]）开头和结尾，并带有破折号（-），下划线（_），点（。），以及之间的字母数字 。 前缀是可选的。 如果指定，则前缀必须是DNS子域：一系列由点（。）分隔的DNS标签，总计不超过253个字符，后跟斜杠（/）。 如果省略前缀，则假定注释密钥对用户是私有的。 向最终用户对象添加注释的自动化系统组件（例如kube-scheduler，kube-controller-manager，kube-apiserver，kubectl或其他第三方自动化）必须指定前缀。 kubernetes.io/和k8s.io/前缀是为Kubernetes核心组件保留的。 例如，这是带有注释imageregistry的Pod的配置文件：https://hub.docker.com/： apiVersion: v1 kind: Pod metadata: name: annotations-demo annotations: imageregistry: \"https://hub.docker.com/\" spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 "},"concepts/overview/working-with-objects/field-selectors.html":{"url":"concepts/overview/working-with-objects/field-selectors.html","title":"字段选择器","keywords":"","body":"字段选择器 字段选择器是你能够根据一个或者多个资源字段筛选kubernetes资源。以下是一些实例： metadata.name=my-service metadata.namespace!=default status.phase=Pending 使用kubectl命令筛选status.phase字段值是Running的pod： kubectl get pods --field-selector status.phase=Running 注意：字段选择器本质上是资源过滤器。 默认情况下，不应用选择器/过滤器，这意味着将选择指定类型的所有资源。 这使得kubectl查询kubectl获取pod和kubectl获取pod --field-selector“”等效。 支持的字段 支持的字段选择器因Kubernetes资源类型而异。 所有资源类型都支持metadata.name和metadata.namespace字段。 使用不受支持的字段选择器会产生错误。 例如： kubectl get ingress --field-selector foo.bar=baz Error from server (BadRequest): Unable to find \"ingresses\" that match label selector \"\", field selector \"foo.bar=baz\": \"foo.bar\" is not a known field selector: only \"metadata.name\", \"metadata.namespace\" 支持的操作符 您可以将=，==和！=运算符与字段选择器一起使用（=和==表示同一意思）。 例如，以下kubectl命令选择不在默认名称空间中的所有Kubernetes服务： kubectl get services --all-namespaces --field-selector metadata.namespace!=default 链式选择器 与标签选择器和其他选择器一样，字段选择器可以链接在一起，以逗号分隔的列表形式。 此kubectl命令选择status.phase不等于Running且spec.restartPolicy字段等于Always的所有Pod： kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always 多种资源类型 您可以在多种资源类型之间使用字段选择器。 此kubectl命令选择不在默认名称空间中的所有Statefulsets和Services： kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default "},"concepts/overview/working-with-objects/common-labels.html":{"url":"concepts/overview/working-with-objects/common-labels.html","title":"推荐的标签","keywords":"","body":"推荐的标签 您可以使用比kubectl和仪表板更多的工具来可视化和管理Kubernetes对象。 一组通用标签允许工具互操作，以所有工具都可以理解的通用方式描述对象。 除了支持工具外，推荐标签还以可查询的方式描述了应用程序。 元数据是围绕应用程序的概念组织的。 Kubernetes不是平台即服务（PaaS），并且不具有或不强制实施应用程序的正式概念。 相反，应用程序是非正式的，并使用元数据进行描述。 应用程序包含的内容的定义很松散。 注意：这些是推荐的标签。 它们使管理应用程序变得更加容易，但是任何核心工具都不是必需的。 共享的标签和注释共享一个公共前缀：app.kubernetes.io。 没有前缀的标签是用户专有的。 共享前缀可确保共享标签不会干扰自定义用户标签。 标签 为了充分利用这些标签，应该将它们应用于每个资源对象。 Key Description Example Type app.kubernetes.io/name The name of the application mysql string app.kubernetes.io/instance A unique name identifying the instance of an application mysql-abcxzy string app.kubernetes.io/version The current version of the application (e.g., a semantic version, revision hash, etc.) 5.7.21 string app.kubernetes.io/component The component within the architecture database string app.kubernetes.io/part-of The name of a higher level application this one is part of wordpress string app.kubernetes.io/managed-by The tool being used to manage the operation of an application helm string 为了说明这些标签的作用，请考虑以下StatefulSet对象： apiVersion: apps/v1 kind: StatefulSet metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: mysql-abcxzy app.kubernetes.io/version: \"5.7.21\" app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress app.kubernetes.io/managed-by: helm 应用程序和应用程序实例 一个应用程序可以一次或多次安装到Kubernetes集群中，在某些情况下，可以安装在同一名称空间中。 例如，在不同的网站是WordPress的不同安装的地方，可以多次安装WordPress。 应用程序名称和实例名称分别记录。 例如，WordPress的app.kubernetes.io/name为wordpress，而实例名称为app.kubernetes.io/instance，其值为wordpress-abcxzy。 这使得应用程序和应用程序实例是可识别的。 应用程序的每个实例都必须具有唯一的名称。 示例 为了说明使用这些标签的不同方法，以下示例具有不同的复杂性。 一个简单的无状态服务 考虑使用Deployment和Service对象部署的简单无状态服务的情况。 以下两个片段代表如何以最简单的形式使用标签。 部署用于监视运行应用程序本身的容器。 apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: myservice app.kubernetes.io/instance: myservice-abcxzy ... 使用Service暴露应用： apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: myservice app.kubernetes.io/instance: myservice-abcxzy ... 带有数据库的web应用 考虑一个稍微复杂的应用程序：使用Helm安装的使用数据库（MySQL）的Web应用程序（WordPress）。 以下片段说明了用于部署此应用程序的对象的开始。 以下部署的开始用于WordPress： apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: wordpress app.kubernetes.io/instance: wordpress-abcxzy app.kubernetes.io/version: \"4.9.4\" app.kubernetes.io/managed-by: helm app.kubernetes.io/component: server app.kubernetes.io/part-of: wordpress ... 使用Service暴露应用： apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: wordpress app.kubernetes.io/instance: wordpress-abcxzy app.kubernetes.io/version: \"4.9.4\" app.kubernetes.io/managed-by: helm app.kubernetes.io/component: server app.kubernetes.io/part-of: wordpress ... MySQL作为带有状态数据的StatefulSet公开，它以及它所属的较大应用程序均带有： apiVersion: apps/v1 kind: StatefulSet metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: mysql-abcxzy app.kubernetes.io/version: \"5.7.21\" app.kubernetes.io/managed-by: helm app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress ... 使用Service将mysql暴露成wordpress的部分： apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: mysql-abcxzy app.kubernetes.io/version: \"5.7.21\" app.kubernetes.io/managed-by: helm app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress ... 使用MySQL StatefulSet和Service，您会注意到有关MySQL和WordPress（更广泛的应用程序）的信息。 "},"concepts/architecture/":{"url":"concepts/architecture/","title":"集群架构","keywords":"","body":"集群架构 节点 控制平面与节点通信 控制器 云控制器管理 "},"concepts/architecture/nodes.html":{"url":"concepts/architecture/nodes.html","title":"节点","keywords":"","body":"节点 Kubernetes通过将容器放入Pods中以在Nodes上运行来运行您的工作负载。 取决于群集，节点可以是虚拟机或物理机。 每个节点都由控制平面管理，并包含运行Pods所需的服务 通常，您在一个集群中有多个节点。 在学习或资源有限的环境中，您可能只有一个节点。 节点上的组件包括kubelet，容器运行时和kube-proxy。 管理 将节点添加到API服务器的主要方法有两种： 节点上的kubelet自动注册到控制平面 手动添加Node对象 创建Node对象或节点上的kubelet自注册后，控制平面将检查新的Node对象是否有效。 例如，如果您尝试从以下JSON清单创建节点： { \"kind\": \"Node\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"10.240.79.157\", \"labels\": { \"name\": \"my-first-k8s-node\" } } } Kubernetes在内部创建一个Node对象（表示形式）。 Kubernetes会检查kubelet是否已注册到与Node的metadata.name字段匹配的API服务器。 如果节点运行状况良好（即所有必需的服务都在运行），则可以运行Pod。 否则，该节点的任何群集活动都将被忽略，直到它变得健康为止。 注意： Kubernetes将对象保留为无效的Node并继续检查以查看其是否健康。 您或控制器必须显式删除Node对象才能停止运行状况检查。 Node对象的名称必须是有效的DNS子域名。 节点名称唯一性 该名称标识一个节点。 两个节点不能同时具有相同的名称。 Kubernetes还假定具有相同名称的资源是同一对象。 如果是节点，则隐式假定使用相同名称的实例将具有相同的状态（例如，网络设置，根磁盘内容）。 如果在不更改实例名称的情况下对其进行了修改，则可能导致不一致。 如果需要大量替换或更新Node，则需要先从API服务器中删除现有的Node对象，并在更新后重新添加。 节点自行注册 当kubelet标志--register-node为true（默认设置）时，kubelet将尝试向API服务器注册自身。 这是大多数发行版使用的首选模式。 对于自我注册，kubelet使用以下选项启动： --kubeconfig: 用于向API服务器进行身份验证的凭据的路径 --cloud-provider: 如何与云提供商交谈以读取有关其自身的元数据。 --register-node: 自动向API服务器注册。 --register-with-taints: 用给定的污点列表（用逗号分隔的 = ：）注册该节点。 --node-ip: 节点的IP地址。 --node-labels: 在集群中注册节点时要添加的标签（请参阅由NodeRestriction允许插件实施的标签限制）。 --node-status-update-frequency: 指定kubelet多久将一次节点状态发布到主节点。 启用节点授权模式和NodeRestriction允许插件时，仅授权kubelet创建/修改其自己的Node资源。 手动节点管理 您可以使用kubectl创建和修改Node对象。 当您要手动创建Node对象时，请设置kubelet标志--register-node = false。 无论--register-node的设置如何，都可以修改Node对象。 例如，您可以在现有节点上设置标签或将其标记为不可计划。 您可以结合使用节点上的标签和Pod上的节点选择器来控制调度。 例如，您可以将Pod限制为只能在可用节点的子集上运行。 将节点标记为不可调度可防止调度程序将新的容器放置到该节点上，但不会影响该节点上的现有容器。 这对于节点重新引导或其他维护之前的准备步骤很有用。 要将节点标记为不可调度，请运行： kubectl cordon $NODENAME 注意：作为DaemonSet一部分的Pod允许在无法调度的节点上运行。 守护程序集通常提供应在节点上运行的节点本地服务，即使正在耗尽工作负载应用程序也是如此。 节点状态 节点的状态包含以下信息： 地址 状态 可用性和可分配性 信息 可以使用kubectl查看节点的状态和其他信息：kubectl describe node 输出的每个部分如下所述: 地址 这些字段的用法因您的云提供商或裸机配置而异： HostName： 节点内核报告的主机名。 可以通过kubelet --hostname-override参数覆盖。 ExternalIP： 通常，可外部路由的节点的IP地址（可从群集外部获得） InternalIP： 通常，仅在群集内可路由的节点的IP地址。 状态 条件字段描述所有正在运行的节点的状态。 条件的示例包括： 节点状态 描述 Ready 如果节点运行状况良好并准备好接受Pod，则为True；如果节点运行状况不佳且不接受Pod，则为false；如果节点控制器未从上一个节点监视程序中的节点收到消息，则为Unknown。 -grace-period`（默认值为40秒） DiskPressure 如果磁盘大小存在压力，即磁盘容量低，则为True。 否则为False MemoryPressure 如果节点内存上存在压力（即节点内存不足），则为True。 否则为False PIDPressure 如果进程上存在压力，即节点上的进程太多，则为“真”。 否则为False NetworkUnavailable 如果未正确配置节点的网络，则为True；否则为False。 注意：如果使用命令行工具来打印封闭节点的详细信息，则条件包括SchedulingDisabled。 在Kubernetes API中，SchedulingDisabled不是条件; 相反，警戒节点在其规范中标记为“不可调度”。 节点状态表示为JSON对象。 例如，以下结构描述了一个健康的节点： \"conditions\": [ { \"type\": \"Ready\", \"status\": \"True\", \"reason\": \"KubeletReady\", \"message\": \"kubelet is posting ready status\", \"lastHeartbeatTime\": \"2019-06-05T18:38:35Z\", \"lastTransitionTime\": \"2019-06-05T11:41:27Z\" } ] 如果“就绪”状态的状态保持“未知”或“假”的时间超过pod-eviction-timeout（传递给kube-controller-manager的参数）的时间，则该节点上的所有Pod都已安排好由节点控制器删除。默认逐出超时持续时间为五分钟。在某些情况下，当节点不可访问时，API服务器将无法与节点上的kubelet通信。在重新建立与API服务器的通信之前，无法将删除pod的决定传达给kubelet。同时，计划删除的Pod可能会继续在分区节点上运行。 在确认节点已停止在集群中运行之前，节点控制器不会强制其删除。您可以看到可能在不可达节点上运行的Pod处于“正在终止”或“未知”状态。如果Kubernetes无法从基础架构推断出某个节点永久离开集群的情况，则集群管理员可能需要手动删除该节点对象。从Kubernetes删除节点对象会导致节点上运行的所有Pod对象从API服务器中删除，并释放它们的名称。 节点生命周期控制器会自动创建代表条件的污点。在将Pod分配给节点时，调度程序会考虑节点的污点。 Pod也可以具有容忍度，以使它们可以容忍Node的污点。 有关更多详细信息，请参见按条件污染节点。 可用性和可分配性 描述节点上可用的资源：CPU，内存以及可以调度到节点上的Pod的最大数量。 容量块中的字段指示节点拥有的资源总数。 可分配块指示节点上可供普通Pod消耗的资源量。 在学习如何在节点上保留计算资源的同时，您可以阅读有关容量和可分配资源的更多信息。 信息 描述有关节点的常规信息，例如内核版本，Kubernetes版本（kubelet和kube-proxy版本），Docker版本（如果使用）和操作系统名称。 该信息由Kubelet从节点收集。 节点控制器 节点控制器是Kubernetes控制平面组件，用于管理节点的各个方面。 节点控制器在节点的生命中扮演多个角色。 第一个是在注册节点时将CIDR块分配给该节点（如果CIDR分配已打开）。 第二个是使节点控制器的内部节点列表与云提供商的可用计算机列表保持最新。 在云环境中运行时以及每当节点运行不正常时，节点控制器都会询问云提供商，该节点的VM是否仍然可用。 如果不是，则节点控制器从其节点列表中删除该节点。 第三是监视节点的运行状况。 节点控制器负责： 当节点变得不可访问时，将NodeStatus的NodeReady条件更新为ConditionUnknown，因为节点控制器由于某种原因（例如，节点已关闭）而停止接收心跳。 如果节点仍然无法访问，请使用正常终止从节点中逐出所有Pod。 默认超时为40秒，开始报告ConditionUnknown，之后为5m，开始驱逐Pod。 节点控制器每隔--node-monitor-period秒检查一次每个节点的状态。 心跳 Kubernetes节点发送的心跳有助于确定节点的可用性。 心跳有两种形式：更新NodeStatus和Lease对象。 每个节点在kube-node-lease命名空间中都有一个关联的Lease对象。 租用是一种轻量级的资源，可在群集扩展时提高节点心跳的性能。 kubelet负责创建和更新NodeStatus和Lease对象。 当状态发生变化或在配置的时间间隔内没有更新时，kubelet会更新NodeStatus。 NodeStatus更新的默认间隔为5分钟，比无法访问的节点的40秒默认超时长得多。 Kubelet会每10秒（默认更新间隔）创建并更新其Lease对象。 租赁更新独立于NodeStatus更新发生。 如果“租约”更新失败，则kubelet将从200毫秒开始以指数退避重试，并以7秒为上限。 可靠性 在大多数情况下，节点控制器将逐出速率限制为每秒--node-eviction-rate（默认值为0.1），这意味着每10秒不会从超过1个节点上逐出pod。 当给定可用性区域中的节点不正常时，节点驱逐行为会更改。节点控制器同时检查区域中有多少百分比的节点不正常（NodeReady条件为ConditionUnknown或ConditionFalse）： 如果不健康节点的分数至少为--unhealthy-zone-threshold（默认值为0.55），则驱逐率会降低。 如果集群很小（即具有小于或等于--large-cluster-size-threshold节点-默认值为50），则驱逐将停止。 否则，驱逐速率会降低为--secondary-node-eviction-rate（默认值为0.01）/秒。 每个可用区都实施这些策略的原因是，一个可用区可能会与主分区分开，而其他可用区仍保持连接。如果您的集群没有跨越多个云提供商可用性区域，则只有一个可用性区域（即整个集群）。 将节点分布在各个可用区域上的一个关键原因是，当一个整个区域出现故障时，可以将工作负载转移到运行状况良好的区域。 因此，如果区域中的所有节点都不健康，则节点控制器将以--node-eviction-rate的正常速率逐出。 当所有区域都完全不健康时（即群集中没有健康的节点），便是最极端的情况。 在这种情况下，节点控制器假定主连接存在问题，并停止所有逐出，直到恢复某些连接为止。 节点控制器还负责驱逐在具有NoExecute异味的节点上运行的Pod，除非这些Pod容忍该异味。 节点控制器还会添加与节点问题（例如，节点不可达或未就绪）相对应的污点。 这意味着调度程序不会将Pod放置在不正常的节点上。 警告：kubectl警戒线将一个节点标记为“不可调度”，这具有服务控制器从先前符合资格的任何LoadBalancer节点目标列表中删除该节点的副作用，从而有效地从警戒节点中删除了传入的负载均衡器流量。 节点容量 节点对象跟踪有关节点资源容量的信息：例如，可用内存量和CPU数量。 自行注册的节点会在注册过程中报告其容量。 如果手动添加节点，则在添加节点时需要设置其容量信息。 Kubernetes调度程序确保节点上所有Pod都有足够的资源。 调度程序检查节点上容器请求的总和不大于节点的容量。 该请求总数包括由kubelet管理的所有容器，但不包括由容器运行时直接启动的任何容器，也排除了在kubelet控件之外运行的任何进程。 节点拓扑结构 如果启用了TopologyManager功能门，则kubelet可以在做出资源分配决策时使用拓扑提示。 有关更多信息，请参见控制节点上的拓扑管理策略。 优雅的节点关机 如果启用了GracefulNodeShutdown功能门，则kubelet会尝试检测节点系统关闭并终止在节点上运行的Pod。 Kubelet确保在节点关闭期间，容器遵循正常的容器终止过程。 启用GracefulNodeShutdown功能门后，kubelet将使用systemd抑制器锁定来将节点关闭延迟给定的持续时间。 在关闭期间，kubelet分两个阶段终止pod： 终止在节点上运行的常规Pod。 终止在节点上运行的关键Pod。 优美的节点关闭功能配置了两个KubeletConfiguration选项： ShutdownGracePeriod： 指定节点应延迟关闭的总时间。 这是常规和关键Pod终止Pod的总宽限期。 ShutdownGracePeriodCriticalPods： 指定在节点关闭期间用于终止关键Pod的持续时间。 这应该小于ShutdownGracePeriod。 例如，如果ShutdownGracePeriod = 30s，而ShutdownGracePeriodCriticalPods = 10s，则kubelet将使节点关闭延迟30秒。 在关闭期间，将保留前20（30-10）秒以正常终止正常Pod，而保留最后10秒以终止关键Pod。 "},"concepts/architecture/control-plane-node-communication.html":{"url":"concepts/architecture/control-plane-node-communication.html","title":"控制平面与节点通信","keywords":"","body":"控制平面与节点通信 本文档对控制平面（apiserver）与Kubernetes集群之间的通信路径进行了分类。 目的是允许用户自定义其安装以强化网络配置，以便可以在不受信任的网络上（或在云提供商的完全公共IP上）运行集群。 节点到控制平面 Kubernetes具有“中心辐射型” API模式。 来自节点（或它们运行的Pod）的所有API使用都在apiserver处终止。 其他控制平面组件均未设计为公开远程服务。 apiserver配置为侦听启用了一种或多种形式的客户端身份验证的安全HTTPS端口（通常为443）上的远程连接。 应该启用一种或多种形式的授权，尤其是在允许匿名请求或服务帐户令牌的情况下。 应该为节点配置群集的公共根证书，以便它们可以与有效的客户端凭据一起安全地连接到apiserver。 一个好的方法是提供给kubelet的客户端凭据采用客户端证书的形式。 有关自动配置kubelet客户端证书的信息，请参阅kubelet TLS引导。 希望连接到apiserver的Pod可以通过利用服务帐户来安全地这样做，以便Kubernetes在实例化Pod时会自动将公共根证书和有效的承载令牌注入Pod。 kubernetes服务（默认名称空间）配置有虚拟IP地址，该地址被重定向（通过kube-proxy）到apiserver上的HTTPS端点。 控制平面组件还通过安全端口与群集apiserver通信。 结果，从节点和在节点上运行的Pod到控制平面的连接的默认操作模式在默认情况下是安全的，并且可以在不受信任和/或公共网络上运行。 控制平面到节点 从控制平面（apiserver）到节点有两条主要的通信路径。 第一个是从apiserver到在集群中每个节点上运行的kubelet进程。 第二个是通过apiserver的代理功能从apiserver到任何节点，pod或服务。 apiserver到kubelet 从apiserver到kubelet的连接用于： 正在获取pod的日志。 附加到运行中的pod 提供kubelet的端口转发功能。 这些连接在kubelet的HTTPS端点处终止。 默认情况下，apiserver不会验证kubelet的服务证书，这会使该连接遭受中间人攻击，并且无法在不受信任和/或公共网络上运行。 要验证此连接，请使用--kubelet-certificate-authority标志为apiserver提供一个根证书捆绑包，以用于验证kubelet的服务证书。 如果无法做到这一点，请在apiserver和kubelet之间使用SSH隧道（如果需要），以避免通过不可信或公共网络进行连接。 最后，应该启用Kubelet身份验证和/或授权以保护kubelet API。 apiserver 到节点, pods, 以及services 从apiserver到节点，pod或服务的连接默认为纯HTTP连接，因此未经身份验证或加密。 可以通过在API URL中的节点，pod或服务名称前添加https：来在安全的HTTPS连接上运行它们，但是它们不会验证HTTPS终结点提供的证书，也不会提供客户端凭据。 因此，尽管连接将被加密，但它不会提供任何完整性保证。 这些连接当前在通过不可信或公共网络运行时并不安全。 SSH 隧道 Kubernetes支持SSH隧道以保护控制平面到节点的通信路径。 在此配置中，apiserver启动到群集中每个节点的SSH隧道（连接到侦听端口22的ssh服务器），并通过隧道传递发往kubelet，节点，pod或服务的所有流量。 该隧道确保流量不会暴露在运行节点的网络外部。 SSH隧道目前已被弃用，因此除非您知道自己在做什么，否则不应该选择使用它们。 Konnectivity服务是此通信渠道的替代产品。 Konnectivity 服务 作为SSH隧道的替代，Konnectivity服务为控制平面提供群集通信的TCP级别代理。 Konnectivity服务由两部分组成：控制平面网络中的Konnectivity服务器和节点网络中的Konnectivity代理。 Konnectivity代理启动与Konnectivity服务器的连接并维护网络连接。 启用Konnectivity服务后，所有控制平面到节点的流量都将通过这些连接进行。 访问Konnectivity服务任务以在您的集群中设置Konnectivity服务。 "},"concepts/architecture/controller.html":{"url":"concepts/architecture/controller.html","title":"控制器","keywords":"","body":"控制器 在机器人技术和自动化领域，控制回路是一个非终止回路，用于调节系统状态。 控制回路的一个示例：房间中的恒温器 当您设定温度时，即告诉恒温器您想要的状态。 实际的室温是当前状态。 恒温器通过打开或关闭设备来使当前状态更接近所需状态。 在Kubernetes中，控制器是控制回路，它们监视集群的状态，然后在需要时进行更改或请求更改。 每个控制器都尝试将当前群集状态移动到更接近所需状态。 控制器模式 控制器跟踪至少一种Kubernetes资源类型。 这些对象的spec字段表示所需的状态。 该资源的控制器负责使当前状态更接近于所需状态。 控制器可以自行执行操作； 更常见的是，在Kubernetes中，控制器会将消息发送给具有有用副作用的API服务器。 您将在下面看到此示例。 同过API Server控制 Job控制器是Kubernetes内置控制器的一个示例。 内置控制器通过与集群API服务器进行交互来管理状态。 Job是一个Kubernetes资源，它运行Pod或可能是多个Pod来执行任务然后停止。 一旦调度，Pod对象将成为kubelet所需状态的一部分。 当作业控制器看到一个新任务时，它确保在群集中的某个位置，一组节点上的kubelet运行正确数量的Pod以完成工作。 Job控制器本身不会运行任何Pod或容器。 而是，作业控制器告诉API服务器创建或删除Pod。 控制平面中的其他组件根据新信息起作用（有新的Pod计划和运行），最终工作完成了。 创建新作业后，所需的状态是该作业要完成。 Job控制器使该Job的当前状态更接近于您想要的状态：创建Pod来完成您要对该Job进行的工作，从而使Job接近完成。 控制器还更新配置它们的对象。 例如：完成作业的工作后，作业控制器将更新该作业对象以将其标记为“完成”。 这有点像某些恒温器如何关闭灯以指示您的房间现在处于您设定的温度。 直接控制 与Job相比，某些控制器需要对集群外部的内容进行更改。 例如，如果您使用控制循环来确保集群中有足够的节点，则该控制器需要当前集群之外的内容以在需要时设置新的节点。 与外部状态进行交互的控制器从API服务器中找到所需的状态，然后直接与外部系统进行通信以使当前状态更加紧密。 实际上有一个控制器可以水平扩展集群中的节点 这里的重点是控制器进行一些更改以实现所需的状态，然后将当前状态报告回群集的API服务器。 其他控制回路可以观察报告的数据并采取自己的行动。 在恒温器示例中，如果房间很冷，则另一个控制器可能还会打开防冻加热器。 在Kubernetes集群中，控制平面通过扩展Kubernetes来实现，从而间接与IP地址管理工具，存储服务，云提供商API和其他服务配合使用。 期望状态与当前状态 Kubernetes采用云原生系统视图，并能够处理不断变化的情况。 随着工作的进行，您的集群可能随时随地发生变化，并且控制环会自动修复故障。 这意味着，您的群集可能永远都无法达到稳定状态。 只要您的集群的控制器正在运行并且能够进行有用的更改，总体状态是否稳定都没有关系。 设计 作为其设计宗旨，Kubernetes使用许多控制器，每个控制器管理集群状态的特定方面。 最常见的是，特定的控制环（控制器）使用一种资源作为其所需状态，并使用另一种资源进行管理以使该所需状态发生。 例如，作业的控制器跟踪作业对象（以发现新工作）和Pod对象（以运行作业，然后查看工作何时完成）。 在这种情况下，其他作业将创建作业，而作业控制器将创建Pod。 使用简单的控制器而不是一组相互链接的单片式控制回路很有用。 控制器可能会失败，因此Kubernetes旨在解决这一问题。 可以有多个控制器创建或更新相同类型的对象。 在幕后，Kubernetes控制器确保仅关注与控制资源链接的资源。 例如，您可以具有“部署”和“作业”； 这些都创造了豆荚。 作业控制器不会删除您的Deployment创建的Pod，因为控制器可以使用某些信息（标签）来区分这些Pod。 运行控制器的方式 Kubernetes带有一组在kube-controller-manager内部运行的内置控制器。 这些内置控制器提供了重要的核心行为。 Deployment控制器和Job控制器是Kubernetes本身的一部分（“内置”控制器）控制器的示例。 Kubernetes允许您运行弹性控制平面，这样，如果任何内置控制器发生故障，控制平面的另一部分将接管工作。 您可以找到在控制平面之外运行的控制器来扩展Kubernetes。 或者，如果需要，您可以自己编写一个新的控制器。 您可以将自己的控制器作为一组Pod运行，也可以在Kubernetes外部运行。 最合适的选择取决于特定控制器的功能。 "},"concepts/architecture/cloud-controller.html":{"url":"concepts/architecture/cloud-controller.html","title":"云控制器管理","keywords":"","body":"云控制器管理器 云基础架构基数允许您在公有云，私有云以及混合云上运行kubernetes。kubernetes坚持自动化API驱动，而不是组件间的紧耦合。云控制器管理是一个kubernetes控制平面组件，嵌入到特定云的控制逻辑。云控制器允许您链接集群到云服务提供商的API，以及分离出那些仅仅与集群交互的组件。通过解偶kubernete和底层云基础设施，云管理控制器允许云服务提供商发布不同于kubernetes项目的特性。 云控制器管理器使用插件机制构建，该机制允许不同的云提供商将其平台与Kubernetes集成。 设计 云控制器管理器作为一组重复的进程（通常是Pod中的容器）在控制平面中运行。 每个云控制器管理器在单个过程中实现多个控制器。 注意：您也可以将Kubernetes插件作为云控制器管理器而不是作为控制平面的一部分来运行。 云控制管理器功能 云控制管理器包含以下功能： 节点控制器 在云基础架构中创建新服务器时，节点控制器负责创建Node对象。 节点控制器获取有关在租户内部通过云提供商运行的主机的信息。 节点控制器执行以下功能： 为控制器通过云提供程序API发现的每个服务器初始化一个Node对象。 使用特定于云的信息来注释和标记Node对象，例如，将节点部署到的区域以及其可用资源（CPU，内存等）。 获取节点的主机名和网卡地址 验证节点的运行状况。 万一节点无响应，此控制器将检查您的云提供商的API，以查看服务器是否已被停用/删除/终止。 如果该节点已从云中删除，则控制器将从您的Kubernetes集群中删除Node对象。 一些云提供商的实现将其分为节点控制器和单独的节点生命周期控制器。 路由控制器 路由控制器负责适当地在云中配置路由，以便Kubernetes集群中不同节点上的容器可以相互通信。 取决于云提供商，路由控制器可能还会为Pod网络分配IP地址块。 代理控制器 服务与云基础架构组件集成，例如托管负载平衡器，IP地址，网络数据包筛选和目标运行状况检查。 当声明需要它们的服务资源时，服务控制器会与您的云提供商的API交互以设置负载平衡器和其他基础结构组件。 鉴权 本节将分解云控制器管理器对各种API对象进行执行操作所需的访问权限。 节点控制器 Node控制器仅适用于Node对象。 它需要完全访问权限才能读取和修改Node对象。 v1/Node: Get List Create Update Patch Watch Delete 路由控制器 路由控制器侦听Node对象的创建并适当地配置路由。 它需要获得对Node对象的访问权限。 v1/Node: Get 代理控制器 服务控制器侦听服务对象的Create，Update和Delete事件，然后为这些服务适当配置端点。 要访问服务，需要“列表”和“监视”访问权限。 要更新服务，它需要Patch 和 Update访问权限。 要为服务设置端点资源，它需要访问“创建”，“列表”，“获取”，“监视”和“更新”。 v1/Service: List Get Watch Patch Update 其他 云控制器管理器核心的实现需要访问权限以创建事件对象，并且为了确保安全操作，还需要访问权限以创建ServiceAccounts。 v1/Event: Create Patch Update v1/ServiceAccount: Create 用于云控制器管理器的RBAC ClusterRole如下所示： apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cloud-controller-manager rules: - apiGroups: - \"\" resources: - events verbs: - create - patch - update - apiGroups: - \"\" resources: - nodes verbs: - '*' - apiGroups: - \"\" resources: - nodes/status verbs: - patch - apiGroups: - \"\" resources: - services verbs: - list - patch - update - watch - apiGroups: - \"\" resources: - serviceaccounts verbs: - create - apiGroups: - \"\" resources: - persistentvolumes verbs: - get - list - update - watch - apiGroups: - \"\" resources: - endpoints verbs: - create - get - list - watch - update 下一步 Cloud Controller Manager管理 提供了有关运行和管理Cloud Controller Manager的说明。 "},"concepts/containers/":{"url":"concepts/containers/","title":"容器详解","keywords":"","body":"容器详解 你运行的每一个容器都是可重现的，这种将依赖包含的标准化使得你可以在任何地方运行它并的得到相同的表现。 容器实现了解耦程序应用与基础设施，这使得在不同的云平台和操作系统环境下部署更加容易。 容器镜像 一个容器镜像即是一个准备运行的软件包，包含了运行程序应用的所有东西：代码、一切运行时依赖、程序和系统类库，以及必要的设置的默认值。 根据设计，容器是不可更改的：你不可能修改已经运行的容器的代码。如果你已经有了一个容器化的应用程序，并且想要去做出更改，你需要构建一个新的包含更改的镜像，然后从这个更新的镜像重新创建容器。 容器运行时 容器运行时是一个负责运行容器的软件。 kubernetes支持多种容器运行时：Docker, containerd, CRI-O，以及其他kubernetes CRI的任何实现。 接下来 阅读关于容器镜像的内容 阅读关于pod的内容 "},"concepts/containers/images.html":{"url":"concepts/containers/images.html","title":"镜像","keywords":"","body":"镜像 "},"concepts/containers/container-environment.html":{"url":"concepts/containers/container-environment.html","title":"容器运行环境","keywords":"","body":"容器运行环境 "},"concepts/containers/runtime-class.html":{"url":"concepts/containers/runtime-class.html","title":"运行时类","keywords":"","body":"运行时类 "},"concepts/containers/container-lifecycle-hooks.html":{"url":"concepts/containers/container-lifecycle-hooks.html","title":"容器声明周期钩子","keywords":"","body":"容器声明周期钩子 "},"concepts/workloads/":{"url":"concepts/workloads/","title":"工作负载","keywords":"","body":"工作负载 一个工作负载就是一个运行在kubernetes上的应用程序。不论你的工作负载是单个组件，还是多个协同，在kubernetes中，都是在一个叫做pod的集合中运行。在kubernetes中，一个pod代表集群中的一组运行容器。 kubernetes的pod具有定义好的生命周期。例如，一旦pod在集群中运行，则该pod运行所在的节点上的严重故障意味着该节点上的所有pod均发生故障。kubernetes将该故障级别是为最终级别：即使该节点稍后回复正常，也需要创建一个新的pod来进行恢复。 但是，为了生活轻松，你不需要直接去管理每个pod。作为代替，你可以使用workload资源代表你去管理pod集合。这类资源通过配置控制器保证正在运行的pod的数量正确，达到您的期望。 kubernetes提供几种内置的workload资源： Deployment和ReplicaSet。Deployment是一个管理无状态程序的很好尝试，在Deployment中，任何pod都是可以被随时替换的。 StatefulSet使您可以运行一个或多个以某种方式跟踪状态的相关Pod。 例如，如果您的工作负载持续记录数据，则可以运行将每个Pod与一个PersistentVolume匹配的StatefulSet。 在该StatefulSet的Pod中运行的代码可以将数据复制到同一StatefulSet中的其他Pod，以提高总体弹性。 DaemonSet定义提供节点本地功能的Pod。 这些可能是群集操作的基础（例如，网络插件），或者是附加组件的一部分。每次向集群中添加一个与DaemonSet中的规范匹配的节点时，控制平面都会将该DaemonSet的Pod调度到新节点上。 Job和CronJob定义了运行到完成然后停止的任务。 作业代表一次性任务，而CronJobs则根据计划重复执行。 在更广泛的Kubernetes生态系统中，您可以找到提供其他行为的第三方工作负载资源。 使用自定义资源定义，如果您想要一种不属于Kubernetes核心的特定行为，则可以添加第三方工作负载资源。 例如，如果您想为应用程序运行一组Pod，但是除非所有Pod都可用（否则可能用于某些高吞吐量的分布式任务），否则要停止工作，则可以实现或安装确实提供该功能的扩展。 下一步 除了阅读每种资源之外，您还可以了解与它们相关的特定任务： 使用部署运行无状态应用程序 以单个实例或复制集的形式运行有状态应用程序 使用CronJob运行自动化任务 学习pod知识 "},"concepts/workloads/pods/":{"url":"concepts/workloads/pods/","title":"容器组","keywords":"","body":"容器组 pods是kubernetes中创建和管理的最小单元。一个pod（正如豌豆一样），是一个或者多个容器的组合，它们共享存储和网络资源，以及运行容器的定义。一个pod的内容通常协同落地和协同调度，并且在共享的上下文中运行。一个pod的模版是一个应用的逻辑主机：它包含一个或者多个容器，这些容器紧密的耦合在一起。在非云环境中，在同一物理或虚拟机上执行的应用程序类似于在同一逻辑主机上执行的云应用程序。 正如应用容器一样，pod可以包含在启动时运行的初始化容器。如果集群提供该功能，你可以注入临时容器进行调试。 什么是pod 注意：尽管Kubernetes不仅支持Docker，还支持更多的容器运行时，但Docker是最常见的运行时，它有助于使用Docker中的一些术语来描述Pod。 Pod的共享上下文是一组Linux名称空间，cgroup和潜在的其他隔离方面-与隔离Docker容器相同。 在Pod的上下文中，各个应用程序可能还会应用其他子隔离。 就Docker概念而言，一个Pod类似于一组具有共享名称空间和共享文件系统卷的Docker容器。 使用pod 通常，您无需直接创建Pod，甚至无需创建Pod。 而是使用工作负荷资源（如“部署”或“作业”）创建它们。 如果您的Pod需要跟踪状态，请考虑使用StatefulSet资源。 kubernetes集群中pod主要有两种pod使用方式： pod运行单个容器，“每个容器一个容器”模型是最常见的Kubernetes用例。 在这种情况下，您可以将Pod视为单个容器的包装纸； Kubernetes管理Pod而不是直接管理容器。 运行多个需要协同工作的容器的Pod。 Pod可以封装一个应用程序，该应用程序由紧密关联且需要共享资源的多个位于同一地点的容器组成。 这些位于同一地点的容器形成一个单一的服务单元，例如，一个容器为公众提供存储在共享卷中的数据，而另一个独立的sidecar容器则刷新或更新这些文件。 Pod将这些容器，存储资源和临时网络标识包装为一个单元。 注意：在单个Pod中将多个位于同一地点和受共同管理的容器分组是一个相对高级的用例。 您仅应在容器紧密耦合的特定实例中使用此模式。 每个Pod旨在运行给定应用程序的单个实例。 如果要水平扩展应用程序（通过运行更多实例来提供更多整体资源），则应使用多个Pod，每个实例一个。 在Kubernetes中，这通常称为复制。 通常，工作负载资源及其控制器将复制的Pod作为一个组创建和管理。 有关Kubernetes如何使用工作负载资源及其控制器来实现应用程序扩展和自动修复的更多信息，请参见Pod和控制器。 pod如何管理多个容器 Pod旨在支持形成协作服务单元的多个协作过程（作为容器）。 Pod中的容器会自动位于同一群集中的同一物理或虚拟机上，并在同一物理或虚拟机上进行共同调度。 这些容器可以共享资源和依赖关系，彼此通信，并协调何时以及如何终止它们。 例如，您可能有一个充当共享卷中文件的Web服务器的容器，以及一个单独的“ sidecar”容器，该容器从远程源更新这些文件，如下图所示： 一些Pod具有init容器和app容器。 在启动应用程序容器之前，初始化容器会运行并完成。 Pod在本地为其组成容器提供两种共享资源：网络和存储。 使用pods 您很少会直接在Kubernetes中创建单个Pod，甚至是单独Pod。 这是因为Pod被设计为相对短暂的一次性实体。 当创建Pod（由您直接或由控制器间接创建）时，新的Pod计划在群集中的节点上运行。 Pod会保留在该节点上，直到Pod完成执行，删除Pod对象，由于缺少资源而将Pod逐出或节点发生故障为止。 注意：不要将重新启动Pod中的容器与重新启动Pod混淆。 Pod不是进程，而是用于运行容器的环境。 Pod一直存在直到被删除。 为Pod对象创建清单时，请确保指定的名称是有效的DNS子域名。 pod与控制器 您可以使用工作负载资源为您创建和管理多个Pod。 资源的控制器处理Pod失败时的复制和推出以及自动修复。 例如，如果某个节点发生故障，则控制器会注意到该节点上的Pod已停止工作，并创建了一个替换Pod。 调度程序将替换的Pod放置到健康的节点上。 以下是管理一个或多个Pod的工作负载资源的一些示例： Deployment StatefulSet DaemonSet pod模版 工作负载资源的控制器从Pod模板创建Pod，并代表您管理这些Pod。 PodTemplates是用于创建Pod的规范，并且包含在工作负载资源（如Deployments，Jobs和DaemonSets）中。 工作负载资源的每个控制器都使用工作负载对象内的PodTemplate来创建实际的Pod。 PodTemplate是用于运行应用程序的任何工作负载资源的期望状态的一部分。 下面的示例是一个简单Job的清单，该Job具有一个模板，该模板开始一个容器。 该Pod中的容器会打印一条消息，然后暂停。 apiVersion: batch/v1 kind: Job metadata: name: hello spec: template: # This is the pod template spec: containers: - name: hello image: busybox command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600'] restartPolicy: OnFailure # The pod template ends here 修改Pod模板或切换到新的Pod模板不会对已经存在的Pod产生直接影响。如果更改工作负载资源的Pod模板，则该资源需要创建使用更新后的模板的替换Pod。 例如，StatefulSet控制器确保正在运行的Pod与每个StatefulSet对象的当前Pod模板匹配。如果您编辑StatefulSet以更改其窗格模板，则StatefulSet开始根据更新的模板创建新的Pod。最终，所有旧Pod被新Pod取代，更新完成。 每个工作负载资源都实现自己的规则，以处理Pod模板的更改。如果您想详细了解有关StatefulSet的更多信息，请阅读StatefulSet Basics教程中的更新策略。 在Nodes上，kubelet不会直接观察或管理有关Pod模板和更新的任何详细信息。这些细节被抽象掉了。关注点的抽象和分离简化了系统语义，并使得在不更改现有代码的情况下扩展集群的行为变得可行。 pod的更新与替换 如上一节所述，当更改工作负载资源的Pod模板时，控制器将基于更新的模板创建新的Pod，而不是更新或修补现有Pod。 Kubernetes不会阻止您直接管理Pod。 可以就地更新正在运行的Pod的某些字段。 但是，Pod更新操作（例如补丁和替换）有一些限制： 大部分pod的元数据是不可变的。例如，你不可以修改namespace，name，uid和creationTimestamp字段，generation字段是唯一的，它仅仅接受来自当前值增长的更新。 如果metadata.deletionTimestamp设置了，不允许增加新的键值对到metadata.finalizers列表 pod更新不会修改除了spec.containers[].image，spec.initContainers[].image, spec.activeDeadlineSeconds 或者 spec.tolerations字段；对于spec.tolerations，你仅可以增加新的键值对。 当更新spec.activeDeadlineSeconds字段时，允许两种类型的更新： 将未分配字段设置为正数； 将字段从正数更新为较小的非负数。 资源共享与交互 pod为容器间的数据共享与通信提供了能力。 pod中的存储 Pod可以指定一组共享存储卷。 Pod中的所有容器都可以访问共享卷，从而使这些容器可以共享数据。 卷还允许Pod中的持久数据保留下来，以防其中的容器之一需要重新启动。 有关Kubernetes如何实现共享存储并将其提供给Pods的更多信息，请参见存储。 pod中的网路 每个Pod为每个地址系列分配一个唯一的IP地址。 Pod中的每个容器都共享网络名称空间，包括IP地址和网络端口。 在Pod内部（并且只有那时），属于Pod的容器可以使用localhost相互通信。 当Pod中的容器与Pod外部的实体进行通信时，它们必须协调它们如何使用共享的网络资源（例如端口）。 在Pod中，容器共享一个IP地址和端口空间，并且可以通过本地主机相互查找。 Pod中的容器还可以使用标准的进程间通信（例如SystemV信号量或POSIX共享内存）相互通信。 不同Pod中的容器具有不同的IP地址，并且没有特殊配置就无法通过IPC进行通信。 想要与在其他Pod中运行的容器进行交互的容器可以使用IP网络进行通信。 Pod中的容器将系统主机名视为与Pod的配置名称相同。 在网络部分中有更多关于此的内容。 容器的特权模式 Pod中的任何容器都可以使用容器规范的安全上下文上的特权标志来启用特权模式。 这对于想要使用操作系统管理功能（如操纵网络堆栈或访问硬件设备）的容器很有用。 特权容器内的进程获得的特权几乎与容器外的进程相同。 注意：容器运行时必须支持特权容器的概念才能使此设置相关。 静态pods 静态Pod由特定节点上的kubelet守护程序直接管理，而API服务器不会对其进行观察。 尽管大多数Pod由控制平面（例如，部署）管理，但对于静态Pod，kubelet会直接监督每个静态Pod（如果失败，则将其重新启动）。 静态Pod始终绑定到特定节点上的一个Kubelet。 静态Pod的主要用途是运行一个自托管的控制平面：换句话说，使用kubelet来监督各个控制平面组件。 Kubelet会自动尝试在Kubernetes API服务器上为每个静态Pod创建一个镜像Pod。 这意味着在节点服务器上运行的Pod在API服务器上可见，但无法从该节点进行控制。 下一步 学习pod的生命周期 "},"concepts/workloads/pods/pod-lifecycle.html":{"url":"concepts/workloads/pods/pod-lifecycle.html","title":"容器组声明周期","keywords":"","body":"容器组声明周期 本页介绍pod的生命周期。pods遵循一个定义好的生命周期，一开始是Pending,如果至少一个主容器shunli启动，则转为Running,然后进入Succeeded或者Succeeded状态，这取决于pods中是否有容器处于失败状态。 在运行Pod的同时，kubelet能够重新启动容器以处理某些类型的错误。 在Pod中，Kubernetes跟踪不同的容器状态，并确定采取何种措施使Pod再次健康。 在Kubernetes API中，Pod具有规范和实际状态。 Pod对象的状态由一组Pod条件组成。 您也可以将自定义准备信息插入Pod的条件数据中，如果这对您的应用程序有用的话。 pods在其生命周期内仅调度一次。 将Pod调度（分配）到某个节点后，该Pod将在该Node上运行，直到其停止或终止。 pod生命周期 类似独立的应用容器，Pods被认为是相对短暂的（而不是持久的）实体。 创建Pod，为其分配唯一ID（UID），并将其调度到保留它们的节点，直到终止（根据重新启动策略）或删除为止。 如果某个节点死亡，则安排在该超时时间段后删除该节点的Pod。 pods本身无法自我修复。 如果将Pod调度到随后发生故障的节点，则Pod将被删除；否则，该Pod将被删除。 同样，由于缺乏资源或Node维护，Pod无法幸免。 Kubernetes使用称为控制器的更高级别的抽象，它处理管理相对易用的Pod实例的工作。 给定的Pod（由UID定义）永远不会“重新安排”到其他节点； 取而代之的是，该Pod可以替换为一个新的，几乎相同的Pod，如果需要，甚至可以使用相同的名称，但使用不同的UID。 如果说某个事物与Pod具有相同的生存期（例如体积），则表示事物存在的时间与该特定Pod（具有确切的UID）存在的时间一样长。 如果出于任何原因删除了该Pod，即使创建了相同的替换，也将销毁相关对象（在本示例中为卷）并重新创建。 一个多容器Pod，其中包含一个文件提取器和一个Web服务器，该Web服务器使用持久卷进行容器之间的共享存储。 pod状态 Pod的状态字段是PodStatus对象，该对象具有一个状态字段。 Pod的阶段是Pod在其生命周期中所处位置的简单概括。该阶段既不打算是对容器状态或Pod状态的观察的全面汇总，也不是要成为全面的状态机。 Pod状态的数量和含义受到严格保护。 除了此处记录的内容外，对于具有给定状态的Pod，不应做任何假设。 以下是状态的一些可选值： 值|描述 ---|--- Pending|kubernetes集群接受该pod，但是一个或者多个容器并未准备好。这包括Pod等待排定所花费的时间，以及通过网络下载容器映像所花费的时间。 Running|pod已经绑定到节点。并且所有容器都已经启动，至少一个容器处在运行中，或者处在启动中和重启中。 Succeeded|pod中的所有容器进入销毁，并且不会在此启动。 Failed|Pod中的所有容器均已终止，并且至少一个容器因故障而终止。 也就是说，容器要么以非零状态退出，要么被系统终止。 Unknown|由于某种原因，无法获得Pod的状态。 此阶段通常是由于与Pod应该在其中运行的节点通信时发生错误而发生的。 注意：删除Pod时，某些kubectl命令将其显示为“正在终止”。 此终止状态不是Pod阶段之一。 授予Pod适当终止的期限，默认为30秒。 您可以使用标志--force强制终止Pod。 如果某个节点死亡或与集群的其余部分断开连接，Kubernetes将应用一项策略，将丢失的节点上所有Pod的状态设置为失败。 容器状态 除了Pod的总体阶段之外，Kubernetes还会跟踪Pod内每个容器的状态。 您可以使用容器生命周期挂钩来触发事件，以在容器生命周期中的某些时间点运行。 调度程序将Pod分配给节点后，kubelet将开始使用容器运行时为该Pod创建容器。 有三种可能的容器状态：等待，运行和终止。 要检查Pod容器的状态，可以使用kubectl describe pod 。 输出显示该Pod中每个容器的状态。 每个状态都有特定的含义： Waiting 如果容器未处于“正在运行”或“已终止”状态，则为“正在等待”。 处于等待状态的容器仍在运行，以完成启动所需的操作：例如，从容器映像注册表中提取容器映像，或应用密钥数据。 当您使用kubectl来查询带有Waiting容器的Pod时，您还会看到一个Reason字段来总结为什么容器处于该状态。 Running 正在运行状态表示容器正在执行，没有任何问题。 如果配置了postStart挂钩，则它已经执行并完成。 当您使用kubectl查询带有正在运行的容器的Pod时，您还将看到有关容器何时进入运行状态的信息。 Terminated 处于“已终止”状态的容器开始执行，然后运行完成或由于某种原因而失败。 当您使用kubectl查询带有Terminated容器的Pod时，您会看到一个原因，一个退出代码以及该容器执行期间的开始和结束时间。 如果容器配置了preStop挂钩，则该挂钩将在容器进入“已终止”状态之前运行。 容器重启策略 Pod的规范包含一个restartPolicy字段，其可能值为Always，OnFailure和Never。 默认值为始终。 restartPolicy适用于Pod中的所有容器。 restartPolicy仅指通过kubelet在同一节点上重新启动容器。 在Pod出口中的容器退出后，kubelet会以指数级的退避延迟（10s，20s，40s等）重新启动它们，上限为5分钟。 一旦容器执行了10分钟而没有任何问题，kubelet将重置该容器的重启退避计时器。 pod状况 一个Pod具有PodStatus，该PodStatus具有PodConditions数组，该Pod通过或未通过PodConditions： PodScheduled：已将Pod调度到一个节点。 ContainersReady：Pod中的所有容器均已准备就绪。 Initialized：所有初始化容器已成功启动。 Ready：该Pod能够处理请求，应将其添加到所有匹配服务的负载平衡池中。 字段名称 描述 type pod状况的名称 status 指示该条件是否适用，可能的值为“ True”，“ False”或“ Unknown”。 lastProbeTime 上一次探测Pod条件的时间戳。 lastTransitionTime Pod上一次从一种状态转换为另一种状态的时间戳。 reason 机器语言、大写驼峰字母文本，包含上次状态转换的原因 message 人类可读的消息，指示有关最后状态转换的详细信息。 pod准备就绪 您的应用程序可以向PodStatus：Pod准备就绪中注入额外的反馈或信号。 要使用此功能，请在Pod的规范中设置readinessGates，以指定kubelet为Pod准备状态评估的其他条件列表。 准备就绪取决于Pod的status.condition字段的当前状态。 如果Kubernetes在Pod的status.conditions字段中找不到这样的条件，则该条件的状态默认为“ False”。 以下示例： kind: Pod ... spec: readinessGates: - conditionType: \"www.example.com/feature-1\" status: conditions: - type: Ready # a built in PodCondition status: \"False\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z - type: \"www.example.com/feature-1\" # an extra PodCondition status: \"False\" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z containerStatuses: - containerID: docker://abcd... ready: true ... 您添加的Pod条件必须具有符合Kubernetes标签密钥格式的名称。 pod就绪的状态 kubectl patch命令不支持修补对象状态。 要为Pod设置这些状态条件，应用程序和操作员应使用PATCH操作。 您可以使用Kubernetes客户端库编写为Pod准备状态设置自定义Pod条件的代码。 对于使用自定义条件的Pod，仅当以下两个状态均适用时，该Pod才被评估为就绪： pod中的所有容器准备就绪 所有定义了readinessGates状态的都为true 当Pod的容器准备就绪，但至少缺少一个自定义条件或False时，kubelet将Pod的条件设置为ContainersReady。 容器探针 探针是由容器上的kubelet定期执行的诊断。 为了执行诊断，kubelet调用由容器实现的Handler。 有三种类型的处理程序： ExecAction：在容器内执行指定的命令。 如果命令以状态码0退出，则认为诊断成功。 TCPSocketAction：对指定端口上Pod的IP地址执行TCP检查。 如果端口打开，则认为诊断成功。 HTTPGetAction：对指定端口和路径上Pod的IP地址执行HTTP GET请求。 如果响应的状态码大于或等于200且小于400，则认为诊断成功。 每个探针有3种结果： Success：容器通过了诊断。 Failure：容器诊断失败 Unknown：诊断失败，因此不应采取任何措施。 Kubelet可以选择对正在运行的容器执行三种探测并对其做出反应： livenessProbe：指示容器是否正在运行。 如果活动性探针失败，则kubelet将杀死该容器，并且该容器将接受其重新启动策略。 如果容器不提供活动性探针，则默认状态为“成功”。 readinessProbe：指示容器是否准备好响应请求。 如果就绪探针失败，则端点控制器将从与Pod匹配的所有服务的端点中删除Pod的IP地址。 初始延迟之前的默认就绪状态为“失败”。 如果容器未提供就绪探测器，则默认状态为“成功”。 startupProbe：指示是否启动容器中的应用程序。 如果提供了启动探针，则将禁用所有其他探针，直到成功为止。 如果启动探针失败，则kubelet将杀死该容器，并且该容器将接受其重新启动策略。 如果容器未提供启动探针，则默认状态为“成功”。 有关如何设置活动性，就绪性或启动探针的更多信息，请参阅配置活动性，就绪性和启动探针。 何时适用存活探针 如果您的容器中的过程在遇到问题或变得不正常时能够自行崩溃，则您不一定需要使用活动性探针；否则，无需进行任何操作。 kubelet将根据Pod的restartPolicy自动执行正确的操作。 如果您希望您的容器在探测失败时被杀死并重新启动，请指定一个活动性探测，并指定“ Always”或“ OnFailure”的restartPolicy。 何时适用就绪探针 如果您仅想在探测成功后才开始向Pod发送流量，请指定就绪探测器。 在这种情况下，就绪探针可能与活动探针相同，但是规范中存在就绪探针意味着Pod将在不接收任何流量的情况下启动，并且仅在探针开始成功之后才开始接收流量。 如果您的容器需要在启动过程中加载大型数据，配置文件或迁移，请指定准备情况探针。 如果希望容器能够拆卸下来进行维护，则可以指定一个就绪探针，以检查特定于与活跃探针不同的就绪端点。 注意：如果希望在删除Pod时能够清空请求，则不一定需要准备就绪探针；否则，无需进行任何准备。 删除后，无论是否准备就绪探针，Pod都会自动将自己置于未就绪状态。 在等待Pod中的容器停止时，Pod仍处于未就绪状态。 何时适用启动探针 对于具有需要很长时间才能投入使用的容器的Pod，启动探针非常有用。 您可以配置一个单独的配置来探测容器启动时的情况，而不是设置较长的活动时间间隔，从而允许它花费比活动时间间隔所允许的时间更长的时间。 如果您的容器通常在超过initialDelaySeconds + failureThreshold×periodSeconds的时间内启动，则应指定一个启动探针，该探针检查与活动探针相同的终结点。 periodSeconds的默认值为10s。 然后，应将其failureThreshold设置得足够高，以允许容器启动，而不更改活动性探针的默认值。 这有助于防止死锁。 pod的销毁 因为Pod表示正在集群中的节点上运行的进程，所以重要的是，当不再需要它们时，请允许这些进程正常终止（而不是通过KILL信号突然停止并且没有机会进行清理）。 设计目标是使您能够请求删除并知道进程何时终止，而且还可以确保删除最终完成。 当您请求删除Pod时，集群会记录并跟踪允许的Pod被强制杀死之前的预定宽限期。 有了适当的强制关闭跟踪，kubelet就会尝试正常关闭。 通常，容器运行时将TERM信号发送到每个容器中的主进程。 许多容器运行时都遵循容器映像中定义的STOPSIGNAL值，并发送该值而不是TERM。 一旦宽限期到期，就会将KILL信号发送到所有剩余的进程，然后从API服务器中删除Pod。 如果在等待进程终止时重新启动了kubelet或容器运行时的管理服务，则群集将从一开始重试，包括完整的原始宽限期。 以下是一个流程： 您可以使用kubectl工具以默认的宽限期（30秒）手动删除特定的Pod。 API服务器中的Pod将使用宽限期来更新，超过该时间Pod被视为“死亡”。 如果您使用kubectl describe检查要删除的Pod，则该Pod会显示为“正在终止”。 在Pod运行的节点上：kubelet看到Pod已被标记为终止（设置了正常关闭持续时间）后，kubelet便开始本地Pod关闭过程。 如果Pod的一个容器定义了preStop挂钩，则kubelet会在容器内部运行该挂钩。 如果宽限期到期后preStop挂钩仍在运行，则kubelet要求一次性将宽限期延长2秒。 kubelet触发容器运行时以向每个容器内的进程1发送TERM信号。 在kubelet开始正常关闭的同时，控制平面将从终结点（以及启用了EndpointSlice）的对象中删除该关闭Pod，在这些对象中，这些对象表示已配置选择器的服务。 副本集和其他工作负载资源不再将关闭的Pod视为有效的服务中副本。 终止宽限期一开始，缓慢关闭的Pod就无法继续为流量提供服务，因为负载平衡器（如服务代理）会从端点列表中删除Pod。 当宽限期到期时，kubelet会触发强制关闭。 容器运行时将SIGKILL发送到仍在Pod中任何容器中运行的任何进程。 如果该容器运行时使用一个隐藏的暂停容器，则kubelet还会清除该容器。 通过将宽限期设置为0（立即删除），kubelet触发了从API服务器强制删除Pod对象的操作。 API服务器删除Pod的API对象，然后该对象不再从任何客户端可见。 强制删除pod 注意：强制删除可能会对某些工作负载及其Pod造成破坏。 默认情况下，所有删除均会在30秒内正常显示。 kubectl delete命令支持--grace-period = 选项，该选项可让您覆盖默认值并指定您自己的值。 将宽限期强行设置为0，并立即从API服务器中删除Pod。 如果pod仍在节点上运行，则强制删除会触发kubelet开始立即清除。 注意：必须指定附加标志--force和--grace-period = 0才能执行强制删除。 当执行强制删除时，API服务器不会等待来自kubelet的确认，即Pod已在其运行的节点上终止。 它会立即删除API中的Pod，以便可以使用相同的名称创建一个新的Pod。 在该节点上，被设置为立即终止的Pod在被强制杀死之前，仍将给予一个小的宽限期。 如果需要强制删除StatefulSet中的Pod，请参阅任务文档以从StatefulSet中删除Pod。 失败pod的垃圾回收 对于失败的Pod，API对象将保留在集群的API中，直到人工或控制器进程将其删除为止。 当Pod的数量超过配置的阈值（由kube-controller-manager中的Terminate-pod-gc-threshold确定）时，控制平面将清理终止的Pod（阶段为成功或失败）。 这样可以避免资源泄漏，因为Pod会随着时间的推移创建和终止。 "},"concepts/workloads/pods/init-containers.html":{"url":"concepts/workloads/pods/init-containers.html","title":"初始化容器","keywords":"","body":"初始化容器 Pod中可以有多个运行应用程序的容器，但也可以有一个或多个init容器，这些容器在启动应用程序容器之前就已运行。 初始化容器与常规容器完全一样，除了以下： 初始化容器始终会运行到完成状态。 每个init容器必须成功完成才能启动下一个容器。 如果Pod的初始化容器失败，则kubelet会反复重新启动该初始化容器，直到成功为止。 但是，如果Pod的重启策略为Never，并且初始化容器在该Pod启动期间失败，则Kubernetes会将整个Pod视为失败。 要为Pod指定初始化容器，请将initContainers字段添加到Pod规范中，作为类型为Container的对象的数组，与app container数组一起。 初始化容器的状态在.status.initContainerStatuses字段中作为容器状态的数组返回（类似于.status.containerStatuses字段）。 与常规容器的区别 初始化容器支持应用容器的所有字段和功能，包括资源限制，数量和安全设置。 但是，如参考资料中所述，对初始化容器的资源请求和限制的处理方式有所不同。 另外，初始化容器不支持生命周期，livenessProbe，readinessProbe或startupProbe，因为它们必须运行到Pod就绪才能完成。 如果您为Pod指定了多个初始化容器，则kubelet会依次运行每个初始化容器。 每个初始化容器必须成功，然后才能运行下一个容器。 当所有初始化容器都运行完毕后，kubelet会初始化Pod的应用程序容器并像往常一样运行它们。 使用初始化容器 因为初始化容器的图像与应用程序容器的图像是分开的，所以它们在启动相关代码方面具有一些优势： 初始化容器可以包含应用程序镜像中不存在的实用程序或用于设置的自定义代码。 例如，无需在安装过程中仅使用sed，awk，python或dig之类的工具从另一张镜像制作镜像。 应用镜像构建器和部署者角色可以独立工作，而无需构建成一个镜像 初始化容器可以在与同一Pod中的应用程序容器不同的文件系统视图下运行。 因此，可以授予他们访问应用程序容器无法访问的机密的权限。 由于init容器在任何应用程序容器启动之前便已运行完毕，因此init容器提供了一种机制来阻止或延迟应用程序容器的启动，直到满足一组先决条件为止。 满足先决条件后，即可并行启动Pod中的所有应用程序容器。 初始化容器可以安全地运行实用程序或自定义代码，否则它们会使应用程序镜像的安全性降低。 通过将不必要的工具分开，您可以限制应用程序容器映像的攻击面。 示例 以下示例初始化容器的使用： 等待一个服务被创建，使用类似于以下的命令行shell： for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1 pod注册到远程API上，使用如下命令： curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$()&ip=$()' 启动容器前等待一段时间，如下： sleep 60 下载git仓库到存储卷中 将值放入配置文件中，然后运行模板工具为主应用程序容器动态生成配置文件。 例如，将POD_IP值放置在配置中，然后使用Jinja生成主应用程序配置文件。 使用初始化容器 本示例定义了一个具有两个init容器的简单Pod。 第一个等待myservice，第二个等待mydb。 一旦两个初始化容器都完成，则Pod将从其spec部分运行app容器。 apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox:1.28 command: ['sh', '-c', 'echo The app is running! && sleep 3600'] initContainers: - name: init-myservice image: busybox:1.28 command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"] - name: init-mydb image: busybox:1.28 command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"] 使用以下命令部署pod： kubectl apply -f myapp.yaml 输出类似于： pod/myapp-pod created 通过以下命令检查状态： kubectl get -f myapp.yaml 输出类似如下： NAME READY STATUS RESTARTS AGE myapp-pod 0/1 Init:0/2 0 6m 查看更多详情： kubectl describe -f myapp.yaml 输出类似如下： Name: myapp-pod Namespace: default [...] Labels: app=myapp Status: Pending [...] Init Containers: init-myservice: [...] State: Running [...] init-mydb: [...] State: Waiting Reason: PodInitializing Ready: False [...] Containers: myapp-container: [...] State: Waiting Reason: PodInitializing Ready: False [...] Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 16s 16s 1 {default-scheduler } Normal Scheduled Successfully assigned myapp-pod to 172.17.4.201 16s 16s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulling pulling image \"busybox\" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulled Successfully pulled image \"busybox\" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Created Created container with docker id 5ced34a04634; Security:[seccomp=unconfined] 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Started Started container with docker id 5ced34a04634 查看初始化容器的日志： kubectl logs myapp-pod -c init-myservice # Inspect the first init container kubectl logs myapp-pod -c init-mydb # Inspect the second init container 此时，初始化容器正在等待发现服务mydb和myservice。 使用以下配置使这两个服务显示出来： --- apiVersion: v1 kind: Service metadata: name: myservice spec: ports: - protocol: TCP port: 80 targetPort: 9376 --- apiVersion: v1 kind: Service metadata: name: mydb spec: ports: - protocol: TCP port: 80 targetPort: 9377 kubectl apply -f services.yaml .... 行为的细节 在Pod启动期间，kubelet会延迟运行init容器，直到网络和存储就绪为止。 然后，kubelet按照Pod规范中出现的顺序运行Pod的init容器。 每个初始化容器必须在下一个容器启动之前成功退出。 如果容器由于运行时而无法启动或因失败而退出，则会根据Pod restartPolicy对其进行重试。 但是，如果Pod restartPolicy设置为Always，则初始化容器将使用restartPolicy OnFailure。 在所有初始化容器都成功之前，Pod不能准备就绪。 初始化容器上的端口未在服务下聚合。 正在初始化的Pod处于Pending状态，但应将条件Initialized设置为false。 如果Pod重新启动或重新启动，则所有初始化容器都必须再次执行。 对初始容器规范的更改仅限于容器映像字段。 更改初始化容器映像字段等同于重新启动Pod。 因为初始化容器可以重新启动，重试或重新执行，所以初始化容器代码应该是幂等的。 特别是，应准备在EmptyDirs上写入文件的代码，以防止输出文件已经存在。 初始化容器具有应用程序容器的所有字段。 但是，Kubernetes禁止使用readinessProbe，因为初始化容器无法定义与完成不同的就绪状态。 这是在验证期间强制执行的。 使用Pod上的activeDeadlineSeconds和容器上的livenessProbe可以防止初始化容器永远失败。 有效期限包括初始化容器。 Pod中每个应用程序和init容器的名称必须唯一； 任何与另一个共享名称的容器都会引发验证错误。 资源 给定初始化容器的顺序和执行，适用以下资源使用规则： 在所有初始化容器上定义的任何特定资源请求或限制中的最高者是有效的初始化请求/限制 Pod对资源的有效请求/限制是以下两者中的较高者： 资源的所有应用容器请求/限制的总和 资源的有效初始化请求/限制 调度是基于有效的请求/限制完成的，这意味着init容器可以为Pod生存期内未使用的初始化保留资源。 Pod的有效QoS层的QoS（服务质量）层是用于初始化容器和应用程序容器的QoS层。 根据有效的Pod请求和限制应用配额和限制。 Pod级别控制组（cgroups）基于有效Pod请求和限制，与调度程序相同。 pod重启的原因 pod可能重启，带来初始化容器的重新之心，有以下原因： Pod基础结构容器将重新启动。 这是不常见的，必须由具有根访问节点权限的人来完成。 在将restartPolicy设置为Always的同时，终止Pod中的所有容器，强制重新启动，并且由于垃圾回收而丢失了初始容器完成记录。 更改初始容器映像或由于垃圾回收而丢失了初始容器完成记录时，不会重新启动Pod。 这适用于Kubernetes v1.20及更高版本。 如果您使用的是Kubernetes的早期版本，请查阅所用版本的文档。 "},"concepts/workloads/pods/pod-topology-spread-constraints.html":{"url":"concepts/workloads/pods/pod-topology-spread-constraints.html","title":"Pod拓扑传播限制","keywords":"","body":"Pod拓扑传播限制 您可以使用拓扑扩展约束来控制Pod如何在故障域（例如区域，区域，节点和其他用户定义的拓扑域）之间跨群集分布。 这可以帮助实现高可用性以及有效的资源利用。 注意：在v1.18之前的Kubernetes版本中，必须使用API服务器和调度程序启用EvenPodsSpread功能门，才能使用Pod拓扑扩展约束。 先决条件 节点标签 拓扑扩展约束条件依赖于节点标签来标识每个节点所在的拓扑域。例如，一个节点可能具有以下标签：node = node1，zone = us-east-1a，region = us-east-1 假设，你有一个4节点的集群，附带以下标签： NAME STATUS ROLES AGE VERSION LABELS node1 Ready 4m26s v1.16.0 node=node1,zone=zoneA node2 Ready 3m58s v1.16.0 node=node2,zone=zoneA node3 Ready 3m17s v1.16.0 node=node3,zone=zoneB node4 Ready 2m43s v1.16.0 node=node4,zone=zoneB 集群的逻辑视图如下： ... 除了可以手动设置标签，你也可以复用在大多数集群中流行的知名标签 pods的分发限制 API API字段pod.spec.topologySpreadConstraints定义如下： apiVersion: v1 kind: Pod metadata: name: mypod spec: topologySpreadConstraints: - maxSkew: topologyKey: whenUnsatisfiable: labelSelector: 你可以定义一个或者多个topologySpreadConstraint来告诉kube-scheduler如何将每个传入的pod相对于整个集群的现有pod进行放置。这些字段如下： maxSkew 描述Pod可能不均匀分布的程度。 它是给定拓扑类型的任何两个拓扑域中的匹配Pod数量之间的最大允许差值。 它必须大于零。 根据whenUnsatisfiable的值，其语义也有所不同： 当whenUnsatisfiable的值等于DoNotSchedule的时候，maxSkew是目标拓扑中的匹配吊舱数与全局最小值之间的最大允许差值。 当不满意的时间等于“ ScheduleAnyway”时，调度程序将较高的优先级给予拓扑结构，这将有助于减少时滞。 topologyKey 是节点标签的key。如果两个节点都用此关键字标记并且具有相同的值，则调度程序会将两个节点都视为处于同一拓扑中。 调度程序尝试将平衡数量的Pod放入每个拓扑域中。 whenUnsatisfiable表示如果Pod不满足传播约束，则如何处理： DoNotSchedule（默认）告诉调度程序不要调度它 ScheduleAnyway告诉调度程序在对节点进行优先级划分以最大程度地减少偏斜的同时仍要对其进行调度。 labelSelector用于查找匹配的Pod。 计算与该标签选择器匹配的Pod，以确定其相应拓扑域中的Pod数。 有关更多详细信息，请参见标签选择器。 您可以通过运行kubectl explain Pod.spec.topologySpreadConstraints 来阅读有关此字段的更多信息。 实例： 一种拓扑扩展约束 假设您有一个4节点群集，其中3个标记为foo：bar的Pod分别位于node1，node2和node3中： 。。。 如果我们希望传入的Pod与现有Pod均匀分布在各个区域，则规格可以指定为： pods/topology-spread-constraints/one-constraint.yaml kind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: - name: pause image: k8s.gcr.io/pause:3.1 topologyKey：zone表示均匀分布将仅应用于标签对为“ zone：”的节点。 whenUnsatisfiable：如果传入Pod无法满足约束条件，则DoNotSchedule通知调度程序使其处于待处理状态。 如果调度程序将此传入Pod放入“ zoneA”，则Pods分布将变为[3，1]，因此实际时滞为2（3-1）-这违反了maxSkew：1.在此示例中，传入Pod仅能 放在“ zoneB”上： 您可以调整pod规范，以满足各种要求： 将maxSkew更改为更大的值，例如“ 2”，以便将传入的Pod也可以放置在“ zoneA”上。 将topologyKey更改为“ node”，以便在节点而不是区域之间均匀分布Pod。 在上面的示例中，如果maxSkew保持为“ 1”，则传入的Pod只能放置在“ node4”上 将whenUnsatisfiable：DoNotSchedule更改为whenUnsatisfiable：ScheduleAnyway，以确保传入的Pod始终是可调度的（假设满足其他调度API）。 但是，最好将其放置在匹配Pod较少的拓扑域中。 （请注意，这种可取性已与其他内部调度优先级（如资源使用率等）共同标准化。） 示例：多个拓扑扩展约束 这是建立在前面的例子的基础上的。 假设您有一个4节点群集，其中3个标记为foo：bar的Pod分别位于node1，node2和node3中： 您可以使用2个TopologySpreadConstraints来控制Pod在区域和节点上的扩散： kind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar - maxSkew: 1 topologyKey: node whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: - name: pause image: k8s.gcr.io/pause:3.1 在这种情况下，为了匹配第一个约束，只能将传入的Pod放置在“ zoneB”上； 而就第二个约束而言，传入的Pod只能放置在“ node4”上。 然后将2个约束的结果进行“与”运算，因此唯一可行的选择是将其放置在“ node4”上。 "},"concepts/workloads/pods/disruptions.html":{"url":"concepts/workloads/pods/disruptions.html","title":"分布","keywords":"","body":"破坏 本指南面向希望构建高可用应用的应用管理员，并需要了解什么样的破坏可能发生在pod上。 自愿和非自愿中断 pods不会主动消失，直到某人删除，或者存在不可避免的硬件和系统软件错误。 对于应用来说，我们称呼这些不可避免的场景为非自愿中断。例如： 支持该节点的物理机发生故障 集群管理员失误删除虚拟机实例 云厂商或者hypervisor错误，导致虚拟机实例消失 内核异常 由于网络划分导致节点从集群中消失 由于节点资源不足将pod驱逐 除了资源不足的场景，大多数用户都对所有的场景熟悉，这些并不是kubernete特有的。 我们称呼其他的场景为自愿中断。这些包含程序拥有者的初始化动作，以及集群管理员的初始化动作，典型的程序拥有者的动作包含： 删除deployment或者其他的管理pod的控制器 更新一个deployment导致重启 直接删除一个pod 集群管理员的动作包含： 清空节点以维修或者升级 排空节点，以缩小集群 从一个节点移除pod，以供其他事物使用节点 这些动作可能由集群管理员直接操作，或者由集群管理员定义的自动程序执行，或者由集群主机提供商执行。 请您的群集管理员或咨询您的云提供商或发行文档，以确定是否为您的群集启用了任何自愿中断源。 如果未启用任何一项，则可以跳过创建Pod中断预算的操作。 注意：并非所有自愿中断都受到Pod中断预算的限制。 例如，删除部署或Pod将绕过Pod中断预算。 应对破坏 以下是几种应对被动中断的方法： 保证你的pod请求所需的资源 如果你的程序需要更高的可用性，使用多副本的方式 对于更高的可用性要求，可以将程序分布到跨区域 自愿中断的频率各不相同。 在基本的Kubernetes集群上，没有自动的自愿中断（只有用户触发的中断）。 但是，您的群集管理员或主机提供商可能会运行一些其他服务，这些服务会导致自愿中断。 例如，推出节点软件更新可能会导致自愿中断。 同样，群集（节点）自动缩放的某些实现可能会导致对碎片整理和紧凑型节点的自愿中断。 您的集群管理员或主机提供商应已记录了预期的自愿中断级别（如果有）。 某些配置选项，例如在pod规范中使用PriorityClasses，也可能导致自愿（和非自愿）中断。 pod中断徽章 即便当你面对频繁的主动中断的时候，kubernetes为你运行高可用的程序提供了特性。 作为一个程序的拥有者，你可以为每个应用创建pod中断徽章PDB。PDB限制了副本程序持续的主动中断次数。例如，一个基于quorum的应用程序，希望确保运行中的副本数量永远不会少于quorum需要的数量。一个前端web端点希望确保副本的数量永远不要其余总量的一定百分比。 集群管理员和主机提供商应当使用工具决定遵循哪个PDB，通过调用驱逐API，而不是直接删除pod和deployments。 例如，kubectl drain子命令允许您标记一个node为脱离服务。当你运行kubectl drain的时候，该工具试图驱逐该节点的所有pods。该kubectl以你名义提交的驱逐请求可能会被暂时的拒绝，所以该工具阶段性的重试失败的请求，直到目标节点的所有pod都销毁，或者配置的超时时间到达。 一个PDB指定了一个应用可以容忍副本的数量，相对于它打算拥有的数量。例如，一个deploymen定义.spec.replicas: 5表示在任意给定时间拥有5个副本。如果此处的PDB定义为4个同时，则驱逐API会允许同时发生自愿中断1个。 "},"concepts/workloads/pods/ephemeral-containers.html":{"url":"concepts/workloads/pods/ephemeral-containers.html","title":"临时容器","keywords":"","body":"临时容器 "},"concepts/workloads/controllers/":{"url":"concepts/workloads/controllers/","title":"负载资源","keywords":"","body":"负载资源 "},"concepts/workloads/controllers/deployment.html":{"url":"concepts/workloads/controllers/deployment.html","title":"部署器","keywords":"","body":"部署器 "},"concepts/workloads/controllers/replicaset.html":{"url":"concepts/workloads/controllers/replicaset.html","title":"副本集","keywords":"","body":"副本集 "},"concepts/workloads/controllers/statefulset.html":{"url":"concepts/workloads/controllers/statefulset.html","title":"有状态集","keywords":"","body":"有状态集 "},"concepts/workloads/controllers/daemonset.html":{"url":"concepts/workloads/controllers/daemonset.html","title":"守护集","keywords":"","body":"守护集 "},"concepts/workloads/controllers/job.html":{"url":"concepts/workloads/controllers/job.html","title":"任务","keywords":"","body":"任务 "},"concepts/workloads/controllers/garbage-collection.html":{"url":"concepts/workloads/controllers/garbage-collection.html","title":"垃圾回收","keywords":"","body":"垃圾回收 "},"concepts/workloads/controllers/ttlafterfinished.html":{"url":"concepts/workloads/controllers/ttlafterfinished.html","title":"失效资源的TTL控制器","keywords":"","body":"失效资源的TTL控制器 "},"concepts/workloads/controllers/cron-jobs.html":{"url":"concepts/workloads/controllers/cron-jobs.html","title":"定时任务","keywords":"","body":"定时任务 "},"concepts/workloads/controllers/replicationcontroller.html":{"url":"concepts/workloads/controllers/replicationcontroller.html","title":"副本控制器","keywords":"","body":"副本控制器 "},"concepts/services-networking/":{"url":"concepts/services-networking/","title":"代理、负载均衡与网络","keywords":"","body":"代理、负载均衡与网络 "},"concepts/services-networking/service.html":{"url":"concepts/services-networking/service.html","title":"代理","keywords":"","body":"代理 "},"concepts/services-networking/service-topology.html":{"url":"concepts/services-networking/service-topology.html","title":"代理拓扑","keywords":"","body":"代理拓扑 "},"concepts/services-networking/dns-pod-service.html":{"url":"concepts/services-networking/dns-pod-service.html","title":"代理与pod的DNS","keywords":"","body":"代理与pod的DNS "},"concepts/services-networking/connect-applications-service.html":{"url":"concepts/services-networking/connect-applications-service.html","title":"使用代理访问应用","keywords":"","body":"使用代理访问应用 "},"concepts/services-networking/endpoint-slices.html":{"url":"concepts/services-networking/endpoint-slices.html","title":"端点切片","keywords":"","body":"端点切片 "},"concepts/services-networking/ingress.html":{"url":"concepts/services-networking/ingress.html","title":"入口","keywords":"","body":"入口 "},"concepts/services-networking/ingress-controllers.html":{"url":"concepts/services-networking/ingress-controllers.html","title":"入口控制器","keywords":"","body":"入口控制器 "},"concepts/services-networking/network-policies.html":{"url":"concepts/services-networking/network-policies.html","title":"网络策略","keywords":"","body":"网络策略 "},"concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases.html":{"url":"concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases.html","title":"为pod指定域名解析","keywords":"","body":"为pod指定域名解析 "},"concepts/services-networking/dual-stack.html":{"url":"concepts/services-networking/dual-stack.html","title":"IPv4/IPv6双栈","keywords":"","body":"IPv4/IPv6双栈 "},"concepts/storage/":{"url":"concepts/storage/","title":"后端存储","keywords":"","body":"后端存储 "},"concepts/storage/volumes.html":{"url":"concepts/storage/volumes.html","title":"存储卷","keywords":"","body":"存储卷 "},"concepts/storage/persistent-volumes.html":{"url":"concepts/storage/persistent-volumes.html","title":"持久化卷","keywords":"","body":"持久化卷 "},"concepts/storage/volume-snapshots.html":{"url":"concepts/storage/volume-snapshots.html","title":"存储卷快照","keywords":"","body":"存储卷快照 "},"concepts/storage/volume-pvc-datasource.html":{"url":"concepts/storage/volume-pvc-datasource.html","title":"CSI存储卷克隆","keywords":"","body":"CSI存储卷克隆 "},"concepts/storage/storage-classes.html":{"url":"concepts/storage/storage-classes.html","title":"存储卷类","keywords":"","body":"存储卷类 "},"concepts/storage/volume-snapshot-classes.html":{"url":"concepts/storage/volume-snapshot-classes.html","title":"存储卷快找类","keywords":"","body":"存储卷快找类 "},"concepts/storage/dynamic-provisioning.html":{"url":"concepts/storage/dynamic-provisioning.html","title":"动态存储卷提供商","keywords":"","body":"动态存储卷提供商 "},"concepts/storage/storage-capacity.html":{"url":"concepts/storage/storage-capacity.html","title":"存储大小","keywords":"","body":"存储大小 "},"concepts/storage/ephemeral-volumes.html":{"url":"concepts/storage/ephemeral-volumes.html","title":"临时存储卷","keywords":"","body":"临时存储卷 "},"concepts/storage/storage-limits.html":{"url":"concepts/storage/storage-limits.html","title":"节点特性存储限制","keywords":"","body":"节点特性存储限制 "},"concepts/configuration/":{"url":"concepts/configuration/","title":"配置定义","keywords":"","body":"配置定义 "},"concepts/configuration/overview.html":{"url":"concepts/configuration/overview.html","title":"配置最佳时间","keywords":"","body":"配置最佳时间 "},"concepts/configuration/configmap.html":{"url":"concepts/configuration/configmap.html","title":"配置映射","keywords":"","body":"配置映射 "},"concepts/configuration/secret.html":{"url":"concepts/configuration/secret.html","title":"密钥","keywords":"","body":"密钥 "},"concepts/configuration/manage-resources-containers.html":{"url":"concepts/configuration/manage-resources-containers.html","title":"管理容器配置资源","keywords":"","body":"管理容器配置资源 "},"concepts/configuration/organize-cluster-access-kubeconfig.html":{"url":"concepts/configuration/organize-cluster-access-kubeconfig.html","title":"使用kubeconfig管理集群","keywords":"","body":"使用kubeconfig管理集群 "},"concepts/configuration/pod-priority-preemption.html":{"url":"concepts/configuration/pod-priority-preemption.html","title":"配置节点优先级和抢占","keywords":"","body":"配置节点优先级和抢占 "},"concepts/security/":{"url":"concepts/security/","title":"安全策略","keywords":"","body":"安全策略 "},"concepts/security/overview.html":{"url":"concepts/security/overview.html","title":"云原声安全概览","keywords":"","body":"云原声安全概览 "},"concepts/security/pod-security-standards.html":{"url":"concepts/security/pod-security-standards.html","title":"pod安全标准","keywords":"","body":"pod安全标准 "},"concepts/security/controlling-access.html":{"url":"concepts/security/controlling-access.html","title":"kubernetes API访问控制","keywords":"","body":"kubernetes API访问控制 "},"concepts/policy/":{"url":"concepts/policy/","title":"资源策略","keywords":"","body":"资源策略 "},"concepts/policy/limit-range.html":{"url":"concepts/policy/limit-range.html","title":"限制范围","keywords":"","body":"限制范围 "},"concepts/policy/resource-quotas.html":{"url":"concepts/policy/resource-quotas.html","title":"资源限额","keywords":"","body":"资源限额 "},"concepts/policy/pod-security-policy.html":{"url":"concepts/policy/pod-security-policy.html","title":"pod安全策略","keywords":"","body":"pod安全策略 "},"concepts/policy/pid-limiting.html":{"url":"concepts/policy/pid-limiting.html","title":"进程ID限制与保留","keywords":"","body":"进程ID限制与保留 "},"concepts/scheduling-eviction/":{"url":"concepts/scheduling-eviction/","title":"调度驱逐","keywords":"","body":"调度驱逐 "},"concepts/scheduling-eviction/kube-scheduler.html":{"url":"concepts/scheduling-eviction/kube-scheduler.html","title":"kubernetes调度器","keywords":"","body":"kubernetes调度器 "},"concepts/scheduling-eviction/assign-pod-node.html":{"url":"concepts/scheduling-eviction/assign-pod-node.html","title":"分配POD到节点","keywords":"","body":"分配POD到节点 "},"concepts/scheduling-eviction/resource-bin-packing.html":{"url":"concepts/scheduling-eviction/resource-bin-packing.html","title":"扩展资源垃圾回收","keywords":"","body":"扩展资源垃圾回收 "},"concepts/scheduling-eviction/taint-and-toleration.html":{"url":"concepts/scheduling-eviction/taint-and-toleration.html","title":"污点与容忍","keywords":"","body":"污点与容忍 "},"concepts/scheduling-eviction/pod-overhead.html":{"url":"concepts/scheduling-eviction/pod-overhead.html","title":"pod过载","keywords":"","body":"pod过载 "},"concepts/scheduling-eviction/eviction-policy.html":{"url":"concepts/scheduling-eviction/eviction-policy.html","title":"驱逐策略","keywords":"","body":"驱逐策略 "},"concepts/scheduling-eviction/scheduling-framework.html":{"url":"concepts/scheduling-eviction/scheduling-framework.html","title":"调度框架","keywords":"","body":"调度框架 "},"concepts/scheduling-eviction/scheduler-perf-tuning.html":{"url":"concepts/scheduling-eviction/scheduler-perf-tuning.html","title":"调度程序性能调节","keywords":"","body":"调度程序性能调节 "},"concepts/cluster-administration.html":{"url":"concepts/cluster-administration.html","title":"集群管理","keywords":"","body":"集群管理 "},"concepts/extend-kubernetes.html":{"url":"concepts/extend-kubernetes.html","title":"功能扩展","keywords":"","body":"功能扩展 "},"tasks/":{"url":"tasks/","title":"新手教程","keywords":"","body":"新手教程 "},"tasks/administer-cluste.html":{"url":"tasks/administer-cluste.html","title":"管理集群","keywords":"","body":"管理集群 "},"tasks/configure-pod-container.html":{"url":"tasks/configure-pod-container.html","title":"配置容器","keywords":"","body":"配置容器 "},"tasks/manage-kubernetes-objects.html":{"url":"tasks/manage-kubernetes-objects.html","title":"管理对象","keywords":"","body":"管理对象 "},"tasks/configmap-secret.html":{"url":"tasks/configmap-secret.html","title":"管理密钥","keywords":"","body":"管理密钥 "},"tasks/inject-data-application.html":{"url":"tasks/inject-data-application.html","title":"容器交互","keywords":"","body":"容器交互 "},"tasks/run-application.html":{"url":"tasks/run-application.html","title":"部署服务","keywords":"","body":"部署服务 "},"tasks/job.html":{"url":"tasks/job.html","title":"部署任务","keywords":"","body":"部署任务 "},"tasks/access-application-cluster.html":{"url":"tasks/access-application-cluster.html","title":"访问服务","keywords":"","body":"访问服务 "},"tasks/debug-application-cluster.html":{"url":"tasks/debug-application-cluster.html","title":"监控、审计与调试","keywords":"","body":"监控、审计与调试 "},"tasks/extend-kubernetes.html":{"url":"tasks/extend-kubernetes.html","title":"扩展集群","keywords":"","body":"扩展集群 "},"tasks/tls.html":{"url":"tasks/tls.html","title":"证书加密","keywords":"","body":"证书加密 "},"tasks/manage-daemon.html":{"url":"tasks/manage-daemon.html","title":"守护服务","keywords":"","body":"守护服务 "},"tasks/service-catalog.html":{"url":"tasks/service-catalog.html","title":"服务管理","keywords":"","body":"服务管理 "},"tasks/network.html":{"url":"tasks/network.html","title":"网络实践","keywords":"","body":"网络实践 "},"tasks/kubelet-credential-provider/kubelet-credential-provider.html":{"url":"tasks/kubelet-credential-provider/kubelet-credential-provider.html","title":"配置一个客户端景象认证服务","keywords":"","body":"配置一个客户端景象认证服务 "},"tasks/extend-kubectl/kubectl-plugins.html":{"url":"tasks/extend-kubectl/kubectl-plugins.html","title":"使用插件扩展集群","keywords":"","body":"使用插件扩展集群 "},"tasks/manage-hugepages/scheduling-hugepages.html":{"url":"tasks/manage-hugepages/scheduling-hugepages.html","title":"管理大页码","keywords":"","body":"管理大页码 "},"tasks/manage-gpus/scheduling-gpus.html":{"url":"tasks/manage-gpus/scheduling-gpus.html","title":"调度GPU","keywords":"","body":"调度GPU "}}